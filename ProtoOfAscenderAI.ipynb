{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLAWn2V4ZpIn"
   },
   "source": [
    "# 2.6B transformer based on pile uncopyrighted streamed via huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo5lerbom6dj",
    "outputId": "6f5855ee-fe33-465f-beb2-41184cd9bd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
      "Requirement already satisfied: fsspec[compression] in /usr/local/lib/python3.11/dist-packages (2024.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "\u001b[33mWARNING: fsspec 2024.2.0 does not provide the extra 'compression'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken tqdm datasets tiktoken zstandard \"fsspec[compression]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvKFeUoRngtR"
   },
   "source": [
    "# RESTART THE KERNEL IF RUNNING ON RUNPOD AFTER THIS PIP INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9enT73mnYoiM"
   },
   "outputs": [],
   "source": [
    "# -------- MODEL PARAMS --------\n",
    "n_layers    = 48\n",
    "n_embd      = 2048\n",
    "n_heads     = 16\n",
    "context_len = 1024\n",
    "batch_size  = 8    # with grad accumulation\n",
    "dropout     = 0\n",
    "lr          = 3e-6\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "vocab_size  = tokenizer.n_vocab\n",
    "\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxN4QEpOCCHN"
   },
   "source": [
    "# *fuck this trick i suspended this trick*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AQ3o30qoZIW6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr(step, max_steps, base_lr=lr, warmup_steps=2000):\n",
    "    \"\"\"\n",
    "    Warmup + cosine decay learning rate schedule.\n",
    "\n",
    "    Args:\n",
    "        step (int): current training step\n",
    "        max_steps (int): total training steps\n",
    "        base_lr (float): peak learning rate\n",
    "        warmup_steps (int): number of warmup steps\n",
    "\n",
    "    Returns:\n",
    "        float: learning rate for this step\n",
    "    \"\"\"\n",
    "    return base_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YNClLsseWNq-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "478tzhmPyb8g"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token(x)                    # (B, T, C)\n",
    "        pos = self.pos(torch.arange(T))        # (T, C)\n",
    "        return tok + pos\n",
    "emb = TokenEmbedding()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YL_tG3Xy1Bk"
   },
   "source": [
    "# Above is the definition of our embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ugc-43hy6Ak"
   },
   "source": [
    "# Now define attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tW4hfpmPyh-0"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)                       # (B, T, 3C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # this is the critical line\n",
    "        y = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            is_causal=True,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "        )\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO3ojwF2yvFb"
   },
   "source": [
    "# Constants again and positional embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-TUsNPmp2iWA"
   },
   "outputs": [],
   "source": [
    "class TokenPosEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token(x)                                # (B,T,C)\n",
    "        pos = self.pos(torch.arange(T, device=x.device))  # (T,C)\n",
    "        return tok + pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Jj6mrYi7M9Z"
   },
   "source": [
    "# After attention we have basic FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AIBpH7V57QfZ"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rdJuWGal6g6Z"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n",
    "        self.ff   = FeedForward()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O59GSjd07CcU"
   },
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # token + positional embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block() for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # final normalization + LM head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        # embeddings\n",
    "        tok = self.token_emb(x)                              # (B,T,C)\n",
    "        pos = self.pos_emb(torch.arange(T, device=x.device))# (T,C)\n",
    "        x = tok + pos                                        # (B,T,C)\n",
    "\n",
    "        # APPLY ALL BLOCKS HERE\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # final projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                                # (B,T,vocab)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits = logits.view(B*T, vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w9471Vfr8CrW"
   },
   "outputs": [],
   "source": [
    "model = TransformerLM().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oWWhQhrG8pj8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624.808017 M params\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M params\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xakqTi7NCJsq"
   },
   "source": [
    "# optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "36Uwu8ox8yXJ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2LPa6zEb88eE"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss_from_iters(model, train_batch_iter, val_batch_iter,\n",
    "                             eval_batches=20, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "\n",
    "    for name, it in [(\"train\", train_batch_iter), (\"val\", val_batch_iter)]:\n",
    "        losses = []\n",
    "        for _ in range(eval_batches):\n",
    "            xb, yb = next(it)\n",
    "            # already on device in this pipeline, but keep safe:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[name] = sum(losses) / len(losses)\n",
    "\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zPfmgofSbChr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f179a799e844652ac6d3d158e34a4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65094e96f7ee4000a263996a10aae187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def token_stream(hf_dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Lazily yields lists of token ids from a streaming HF dataset.\n",
    "    \"\"\"\n",
    "    for ex in hf_dataset:\n",
    "        text = ex.get(\"text\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "        ids = tokenizer.encode(text)\n",
    "        if len(ids) > 1:\n",
    "            yield ids\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def window_stream(token_iter, context_len, device):\n",
    "    \"\"\"\n",
    "    Yields single (x, y) training samples of shape [context_len].\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "\n",
    "    for ids in token_iter:\n",
    "        buffer.extend(ids)\n",
    "\n",
    "        while len(buffer) >= context_len + 1:\n",
    "            start = random.randint(0, len(buffer) - context_len - 1)\n",
    "\n",
    "            x = buffer[start : start + context_len]\n",
    "            y = buffer[start + 1 : start + context_len + 1]\n",
    "\n",
    "            yield (\n",
    "                torch.tensor(x, dtype=torch.long, device=device),\n",
    "                torch.tensor(y, dtype=torch.long, device=device),\n",
    "            )\n",
    "def batch_stream(sample_iter, batch_size):\n",
    "    \"\"\"\n",
    "    Groups single samples into batches.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        xb, yb = zip(*(next(sample_iter) for _ in range(batch_size)))\n",
    "        yield torch.stack(xb), torch.stack(yb)\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load streaming dataset\n",
    "ds = load_dataset(\n",
    "    \"monology/pile-uncopyrighted\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "\n",
    "# build streams\n",
    "tok_iter   = token_stream(ds, tokenizer)\n",
    "sample_iter = window_stream(tok_iter, context_len, device)\n",
    "batch_iter  = batch_stream(sample_iter, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_my_token\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/345rf4gt56t4r3e3/nnn', endpoint='https://huggingface.co', repo_type='model', repo_id='345rf4gt56t4r3e3/nnn')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, upload_file\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "HF_REPO = \"345rf4gt56t4r3e3/nnn\"\n",
    "CKPT_DIR = \"checkpoints\"\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# create repo if it doesn't exist\n",
    "api.create_repo(\n",
    "    repo_id=HF_REPO,\n",
    "    exist_ok=True,\n",
    "    repo_type=\"model\",\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING STREAMS ===\n",
      "[build_streaming_splits] loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64103214b614dc4aa3c00da53115c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build_streaming_splits] dataset object ready in 1.46s\n",
      "[build_streaming_splits] building train stream...\n",
      "[build_streaming_splits] train stream ready in 0.00s\n",
      "[build_streaming_splits] building val stream (skip with progress)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507b71a2f0c34fdbb561a855cae0980c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val.skip] 1/2,000,000 | inst=1.2 ex/s avg=1.2 ex/s | elapsed=0.0m eta=27371.0m\n",
      "[val.skip] 2/2,000,000 | inst=14,218.0 ex/s avg=2.4 ex/s | elapsed=0.0m eta=13686.7m\n",
      "[val.skip] 3/2,000,000 | inst=80,659.7 ex/s avg=3.7 ex/s | elapsed=0.0m eta=9124.6m\n",
      "[val.skip] 4/2,000,000 | inst=127,100.1 ex/s avg=4.9 ex/s | elapsed=0.0m eta=6843.5m\n",
      "[val.skip] 5/2,000,000 | inst=76,260.1 ex/s avg=6.1 ex/s | elapsed=0.0m eta=5474.9m\n",
      "[val.skip] 6/2,000,000 | inst=131,072.0 ex/s avg=7.3 ex/s | elapsed=0.0m eta=4562.4m\n",
      "[val.skip] 7/2,000,000 | inst=119,837.3 ex/s avg=8.5 ex/s | elapsed=0.0m eta=3910.7m\n",
      "[val.skip] 8/2,000,000 | inst=87,381.3 ex/s avg=9.7 ex/s | elapsed=0.0m eta=3421.9m\n",
      "[val.skip] 9/2,000,000 | inst=116,508.4 ex/s avg=11.0 ex/s | elapsed=0.0m eta=3041.7m\n",
      "[val.skip] 10/2,000,000 | inst=135,300.1 ex/s avg=12.2 ex/s | elapsed=0.0m eta=2737.6m\n",
      "[val.skip] 10,000/2,000,000 | inst=9,068.8 ex/s avg=5,200.6 ex/s | elapsed=0.0m eta=6.4m\n",
      "[val.skip] 20,000/2,000,000 | inst=9,133.5 ex/s avg=6,627.5 ex/s | elapsed=0.1m eta=5.0m\n",
      "[val.skip] 30,000/2,000,000 | inst=8,090.4 ex/s avg=7,052.6 ex/s | elapsed=0.1m eta=4.7m\n",
      "[val.skip] 40,000/2,000,000 | inst=10,686.5 ex/s avg=7,707.8 ex/s | elapsed=0.1m eta=4.2m\n",
      "[val.skip] 50,000/2,000,000 | inst=11,351.2 ex/s avg=8,236.6 ex/s | elapsed=0.1m eta=3.9m\n",
      "[val.skip] 60,000/2,000,000 | inst=8,621.2 ex/s avg=8,298.3 ex/s | elapsed=0.1m eta=3.9m\n",
      "[val.skip] 70,000/2,000,000 | inst=10,242.3 ex/s avg=8,529.5 ex/s | elapsed=0.1m eta=3.8m\n",
      "[val.skip] 80,000/2,000,000 | inst=7,841.3 ex/s avg=8,437.0 ex/s | elapsed=0.2m eta=3.8m\n",
      "[val.skip] 90,000/2,000,000 | inst=9,735.4 ex/s avg=8,563.9 ex/s | elapsed=0.2m eta=3.7m\n",
      "[val.skip] 100,000/2,000,000 | inst=8,103.0 ex/s avg=8,515.5 ex/s | elapsed=0.2m eta=3.7m\n",
      "[val.skip] 110,000/2,000,000 | inst=6,451.9 ex/s avg=8,274.9 ex/s | elapsed=0.2m eta=3.8m\n",
      "[val.skip] 120,000/2,000,000 | inst=8,307.3 ex/s avg=8,277.6 ex/s | elapsed=0.2m eta=3.8m\n",
      "[val.skip] 130,000/2,000,000 | inst=9,495.6 ex/s avg=8,360.0 ex/s | elapsed=0.3m eta=3.7m\n",
      "[val.skip] 140,000/2,000,000 | inst=8,369.2 ex/s avg=8,360.7 ex/s | elapsed=0.3m eta=3.7m\n",
      "[val.skip] 150,000/2,000,000 | inst=10,384.9 ex/s avg=8,470.8 ex/s | elapsed=0.3m eta=3.6m\n",
      "[val.skip] 160,000/2,000,000 | inst=8,389.8 ex/s avg=8,465.7 ex/s | elapsed=0.3m eta=3.6m\n",
      "[val.skip] 170,000/2,000,000 | inst=8,968.2 ex/s avg=8,493.7 ex/s | elapsed=0.3m eta=3.6m\n",
      "[val.skip] 180,000/2,000,000 | inst=7,874.3 ex/s avg=8,456.7 ex/s | elapsed=0.4m eta=3.6m\n",
      "[val.skip] 190,000/2,000,000 | inst=9,940.0 ex/s avg=8,523.6 ex/s | elapsed=0.4m eta=3.5m\n",
      "[val.skip] 200,000/2,000,000 | inst=7,308.8 ex/s avg=8,453.4 ex/s | elapsed=0.4m eta=3.5m\n",
      "[val.skip] 210,000/2,000,000 | inst=6,704.7 ex/s avg=8,349.7 ex/s | elapsed=0.4m eta=3.6m\n",
      "[val.skip] 220,000/2,000,000 | inst=10,374.0 ex/s avg=8,424.4 ex/s | elapsed=0.4m eta=3.5m\n",
      "[val.skip] 230,000/2,000,000 | inst=10,204.2 ex/s avg=8,488.8 ex/s | elapsed=0.5m eta=3.5m\n",
      "[val.skip] 240,000/2,000,000 | inst=7,848.8 ex/s avg=8,460.0 ex/s | elapsed=0.5m eta=3.5m\n",
      "[val.skip] 250,000/2,000,000 | inst=8,009.7 ex/s avg=8,441.1 ex/s | elapsed=0.5m eta=3.5m\n",
      "[val.skip] 260,000/2,000,000 | inst=10,043.1 ex/s avg=8,493.2 ex/s | elapsed=0.5m eta=3.4m\n",
      "[val.skip] 270,000/2,000,000 | inst=7,908.8 ex/s avg=8,470.0 ex/s | elapsed=0.5m eta=3.4m\n",
      "[val.skip] 280,000/2,000,000 | inst=1,644.6 ex/s avg=7,376.6 ex/s | elapsed=0.6m eta=3.9m\n",
      "[val.skip] 290,000/2,000,000 | inst=8,334.5 ex/s avg=7,406.0 ex/s | elapsed=0.7m eta=3.8m\n",
      "[val.skip] 300,000/2,000,000 | inst=8,226.6 ex/s avg=7,430.7 ex/s | elapsed=0.7m eta=3.8m\n",
      "[val.skip] 310,000/2,000,000 | inst=6,303.0 ex/s avg=7,388.1 ex/s | elapsed=0.7m eta=3.8m\n",
      "[val.skip] 320,000/2,000,000 | inst=8,403.3 ex/s avg=7,416.1 ex/s | elapsed=0.7m eta=3.8m\n",
      "[val.skip] 330,000/2,000,000 | inst=7,389.5 ex/s avg=7,415.2 ex/s | elapsed=0.7m eta=3.8m\n",
      "[val.skip] 340,000/2,000,000 | inst=6,778.4 ex/s avg=7,394.8 ex/s | elapsed=0.8m eta=3.7m\n",
      "[val.skip] 350,000/2,000,000 | inst=7,332.0 ex/s avg=7,393.0 ex/s | elapsed=0.8m eta=3.7m\n",
      "[val.skip] 360,000/2,000,000 | inst=4,940.9 ex/s avg=7,292.5 ex/s | elapsed=0.8m eta=3.7m\n",
      "[val.skip] 370,000/2,000,000 | inst=7,172.8 ex/s avg=7,289.2 ex/s | elapsed=0.8m eta=3.7m\n",
      "[val.skip] 380,000/2,000,000 | inst=3,529.8 ex/s avg=7,090.5 ex/s | elapsed=0.9m eta=3.8m\n",
      "[val.skip] 390,000/2,000,000 | inst=8,132.8 ex/s avg=7,113.8 ex/s | elapsed=0.9m eta=3.8m\n",
      "[val.skip] 400,000/2,000,000 | inst=2,139.8 ex/s avg=6,723.1 ex/s | elapsed=1.0m eta=4.0m\n",
      "[val.skip] 410,000/2,000,000 | inst=7,908.1 ex/s avg=6,747.8 ex/s | elapsed=1.0m eta=3.9m\n",
      "[val.skip] 420,000/2,000,000 | inst=6,488.7 ex/s avg=6,741.4 ex/s | elapsed=1.0m eta=3.9m\n",
      "[val.skip] 430,000/2,000,000 | inst=8,547.6 ex/s avg=6,774.7 ex/s | elapsed=1.1m eta=3.9m\n",
      "[val.skip] 440,000/2,000,000 | inst=6,535.1 ex/s avg=6,769.0 ex/s | elapsed=1.1m eta=3.8m\n",
      "[val.skip] 450,000/2,000,000 | inst=6,635.5 ex/s avg=6,766.0 ex/s | elapsed=1.1m eta=3.8m\n",
      "[val.skip] 460,000/2,000,000 | inst=7,667.5 ex/s avg=6,783.4 ex/s | elapsed=1.1m eta=3.8m\n",
      "[val.skip] 470,000/2,000,000 | inst=5,796.9 ex/s avg=6,758.9 ex/s | elapsed=1.2m eta=3.8m\n",
      "[val.skip] 480,000/2,000,000 | inst=7,822.3 ex/s avg=6,778.1 ex/s | elapsed=1.2m eta=3.7m\n",
      "[val.skip] 490,000/2,000,000 | inst=6,897.2 ex/s avg=6,780.5 ex/s | elapsed=1.2m eta=3.7m\n",
      "[val.skip] 500,000/2,000,000 | inst=6,275.0 ex/s avg=6,769.6 ex/s | elapsed=1.2m eta=3.7m\n",
      "[val.skip] 510,000/2,000,000 | inst=7,336.1 ex/s avg=6,779.8 ex/s | elapsed=1.3m eta=3.7m\n",
      "[val.skip] 520,000/2,000,000 | inst=6,130.8 ex/s avg=6,766.1 ex/s | elapsed=1.3m eta=3.6m\n",
      "[val.skip] 530,000/2,000,000 | inst=4,980.7 ex/s avg=6,720.6 ex/s | elapsed=1.3m eta=3.6m\n",
      "[val.skip] 540,000/2,000,000 | inst=5,805.5 ex/s avg=6,701.0 ex/s | elapsed=1.3m eta=3.6m\n",
      "[val.skip] 550,000/2,000,000 | inst=6,925.3 ex/s avg=6,705.0 ex/s | elapsed=1.4m eta=3.6m\n",
      "[val.skip] 560,000/2,000,000 | inst=6,587.9 ex/s avg=6,702.9 ex/s | elapsed=1.4m eta=3.6m\n",
      "[val.skip] 570,000/2,000,000 | inst=7,365.9 ex/s avg=6,713.5 ex/s | elapsed=1.4m eta=3.6m\n",
      "[val.skip] 580,000/2,000,000 | inst=6,705.1 ex/s avg=6,713.3 ex/s | elapsed=1.4m eta=3.5m\n",
      "[val.skip] 590,000/2,000,000 | inst=7,685.5 ex/s avg=6,727.7 ex/s | elapsed=1.5m eta=3.5m\n",
      "[val.skip] 600,000/2,000,000 | inst=3,732.4 ex/s avg=6,638.9 ex/s | elapsed=1.5m eta=3.5m\n",
      "[val.skip] 610,000/2,000,000 | inst=2,241.6 ex/s avg=6,432.1 ex/s | elapsed=1.6m eta=3.6m\n",
      "[val.skip] 620,000/2,000,000 | inst=8,311.7 ex/s avg=6,455.6 ex/s | elapsed=1.6m eta=3.6m\n",
      "[val.skip] 630,000/2,000,000 | inst=6,526.1 ex/s avg=6,456.7 ex/s | elapsed=1.6m eta=3.5m\n",
      "[val.skip] 640,000/2,000,000 | inst=6,553.3 ex/s avg=6,458.2 ex/s | elapsed=1.7m eta=3.5m\n",
      "[val.skip] 650,000/2,000,000 | inst=8,090.8 ex/s avg=6,478.3 ex/s | elapsed=1.7m eta=3.5m\n",
      "[val.skip] 660,000/2,000,000 | inst=6,488.7 ex/s avg=6,478.5 ex/s | elapsed=1.7m eta=3.4m\n",
      "[val.skip] 670,000/2,000,000 | inst=8,419.1 ex/s avg=6,500.9 ex/s | elapsed=1.7m eta=3.4m\n",
      "[val.skip] 680,000/2,000,000 | inst=7,020.5 ex/s avg=6,508.0 ex/s | elapsed=1.7m eta=3.4m\n",
      "[val.skip] 690,000/2,000,000 | inst=6,214.0 ex/s avg=6,503.5 ex/s | elapsed=1.8m eta=3.4m\n",
      "[val.skip] 700,000/2,000,000 | inst=8,112.3 ex/s avg=6,522.0 ex/s | elapsed=1.8m eta=3.3m\n",
      "[val.skip] 710,000/2,000,000 | inst=6,725.8 ex/s avg=6,524.8 ex/s | elapsed=1.8m eta=3.3m\n",
      "[val.skip] 720,000/2,000,000 | inst=8,209.9 ex/s avg=6,543.4 ex/s | elapsed=1.8m eta=3.3m\n",
      "[val.skip] 730,000/2,000,000 | inst=6,271.0 ex/s avg=6,539.5 ex/s | elapsed=1.9m eta=3.2m\n",
      "[val.skip] 740,000/2,000,000 | inst=7,841.4 ex/s avg=6,554.2 ex/s | elapsed=1.9m eta=3.2m\n",
      "[val.skip] 750,000/2,000,000 | inst=6,823.3 ex/s avg=6,557.7 ex/s | elapsed=1.9m eta=3.2m\n",
      "[val.skip] 760,000/2,000,000 | inst=7,929.5 ex/s avg=6,572.6 ex/s | elapsed=1.9m eta=3.1m\n",
      "[val.skip] 770,000/2,000,000 | inst=4,093.0 ex/s avg=6,521.3 ex/s | elapsed=2.0m eta=3.1m\n",
      "[val.skip] 780,000/2,000,000 | inst=7,982.0 ex/s avg=6,536.7 ex/s | elapsed=2.0m eta=3.1m\n",
      "[val.skip] 790,000/2,000,000 | inst=5,927.4 ex/s avg=6,528.2 ex/s | elapsed=2.0m eta=3.1m\n",
      "[val.skip] 800,000/2,000,000 | inst=5,928.4 ex/s avg=6,519.9 ex/s | elapsed=2.0m eta=3.1m\n",
      "[val.skip] 810,000/2,000,000 | inst=7,223.9 ex/s avg=6,527.8 ex/s | elapsed=2.1m eta=3.0m\n",
      "[val.skip] 820,000/2,000,000 | inst=6,808.7 ex/s avg=6,531.1 ex/s | elapsed=2.1m eta=3.0m\n",
      "[val.skip] 830,000/2,000,000 | inst=7,025.3 ex/s avg=6,536.6 ex/s | elapsed=2.1m eta=3.0m\n",
      "[val.skip] 840,000/2,000,000 | inst=3,719.4 ex/s avg=6,478.2 ex/s | elapsed=2.2m eta=3.0m\n",
      "[val.skip] 850,000/2,000,000 | inst=6,930.9 ex/s avg=6,483.2 ex/s | elapsed=2.2m eta=3.0m\n",
      "[val.skip] 860,000/2,000,000 | inst=5,382.2 ex/s avg=6,467.8 ex/s | elapsed=2.2m eta=2.9m\n",
      "[val.skip] 870,000/2,000,000 | inst=8,159.4 ex/s avg=6,483.2 ex/s | elapsed=2.2m eta=2.9m\n",
      "[val.skip] 880,000/2,000,000 | inst=6,123.3 ex/s avg=6,478.9 ex/s | elapsed=2.3m eta=2.9m\n",
      "[val.skip] 890,000/2,000,000 | inst=8,221.9 ex/s avg=6,494.4 ex/s | elapsed=2.3m eta=2.8m\n",
      "[val.skip] 900,000/2,000,000 | inst=6,677.1 ex/s avg=6,496.4 ex/s | elapsed=2.3m eta=2.8m\n",
      "[val.skip] 910,000/2,000,000 | inst=8,363.3 ex/s avg=6,512.3 ex/s | elapsed=2.3m eta=2.8m\n",
      "[val.skip] 920,000/2,000,000 | inst=6,329.6 ex/s avg=6,510.3 ex/s | elapsed=2.4m eta=2.8m\n",
      "[val.skip] 930,000/2,000,000 | inst=8,133.7 ex/s avg=6,524.3 ex/s | elapsed=2.4m eta=2.7m\n",
      "[val.skip] 940,000/2,000,000 | inst=7,054.1 ex/s avg=6,529.5 ex/s | elapsed=2.4m eta=2.7m\n",
      "[val.skip] 950,000/2,000,000 | inst=6,537.3 ex/s avg=6,529.6 ex/s | elapsed=2.4m eta=2.7m\n",
      "[val.skip] 960,000/2,000,000 | inst=7,958.5 ex/s avg=6,541.8 ex/s | elapsed=2.4m eta=2.6m\n",
      "[val.skip] 970,000/2,000,000 | inst=6,749.8 ex/s avg=6,543.9 ex/s | elapsed=2.5m eta=2.6m\n",
      "[val.skip] 980,000/2,000,000 | inst=8,590.5 ex/s avg=6,559.8 ex/s | elapsed=2.5m eta=2.6m\n",
      "[val.skip] 990,000/2,000,000 | inst=6,554.8 ex/s avg=6,559.8 ex/s | elapsed=2.5m eta=2.6m\n",
      "[val.skip] 1,000,000/2,000,000 | inst=5,332.1 ex/s avg=6,544.7 ex/s | elapsed=2.5m eta=2.5m\n",
      "[val.skip] 1,010,000/2,000,000 | inst=6,746.0 ex/s avg=6,546.7 ex/s | elapsed=2.6m eta=2.5m\n",
      "[val.skip] 1,020,000/2,000,000 | inst=8,323.2 ex/s avg=6,560.4 ex/s | elapsed=2.6m eta=2.5m\n",
      "[val.skip] 1,030,000/2,000,000 | inst=6,231.6 ex/s avg=6,557.0 ex/s | elapsed=2.6m eta=2.5m\n",
      "[val.skip] 1,040,000/2,000,000 | inst=7,603.2 ex/s avg=6,565.7 ex/s | elapsed=2.6m eta=2.4m\n",
      "[val.skip] 1,050,000/2,000,000 | inst=5,302.7 ex/s avg=6,550.9 ex/s | elapsed=2.7m eta=2.4m\n",
      "[val.skip] 1,060,000/2,000,000 | inst=7,087.6 ex/s avg=6,555.5 ex/s | elapsed=2.7m eta=2.4m\n",
      "[val.skip] 1,070,000/2,000,000 | inst=5,354.5 ex/s avg=6,541.8 ex/s | elapsed=2.7m eta=2.4m\n",
      "[val.skip] 1,080,000/2,000,000 | inst=8,572.9 ex/s avg=6,556.2 ex/s | elapsed=2.7m eta=2.3m\n",
      "[val.skip] 1,090,000/2,000,000 | inst=6,640.8 ex/s avg=6,557.0 ex/s | elapsed=2.8m eta=2.3m\n",
      "[val.skip] 1,100,000/2,000,000 | inst=8,503.9 ex/s avg=6,570.7 ex/s | elapsed=2.8m eta=2.3m\n",
      "[val.skip] 1,110,000/2,000,000 | inst=6,858.3 ex/s avg=6,573.1 ex/s | elapsed=2.8m eta=2.3m\n",
      "[val.skip] 1,120,000/2,000,000 | inst=6,245.7 ex/s avg=6,570.1 ex/s | elapsed=2.8m eta=2.2m\n",
      "[val.skip] 1,130,000/2,000,000 | inst=8,046.9 ex/s avg=6,580.7 ex/s | elapsed=2.9m eta=2.2m\n",
      "[val.skip] 1,140,000/2,000,000 | inst=6,524.8 ex/s avg=6,580.3 ex/s | elapsed=2.9m eta=2.2m\n",
      "[val.skip] 1,150,000/2,000,000 | inst=6,641.8 ex/s avg=6,580.8 ex/s | elapsed=2.9m eta=2.2m\n",
      "[val.skip] 1,160,000/2,000,000 | inst=8,132.6 ex/s avg=6,591.6 ex/s | elapsed=2.9m eta=2.1m\n",
      "[val.skip] 1,170,000/2,000,000 | inst=6,540.2 ex/s avg=6,591.2 ex/s | elapsed=3.0m eta=2.1m\n",
      "[val.skip] 1,180,000/2,000,000 | inst=6,961.9 ex/s avg=6,594.2 ex/s | elapsed=3.0m eta=2.1m\n",
      "[val.skip] 1,190,000/2,000,000 | inst=8,519.7 ex/s avg=6,606.7 ex/s | elapsed=3.0m eta=2.0m\n",
      "[val.skip] 1,200,000/2,000,000 | inst=6,494.3 ex/s avg=6,605.8 ex/s | elapsed=3.0m eta=2.0m\n",
      "[val.skip] 1,210,000/2,000,000 | inst=1,956.4 ex/s avg=6,478.5 ex/s | elapsed=3.1m eta=2.0m\n",
      "[val.skip] 1,220,000/2,000,000 | inst=6,630.5 ex/s avg=6,479.7 ex/s | elapsed=3.1m eta=2.0m\n",
      "[val.skip] 1,230,000/2,000,000 | inst=1,512.8 ex/s avg=6,311.3 ex/s | elapsed=3.2m eta=2.0m\n",
      "[val.skip] 1,240,000/2,000,000 | inst=6,555.8 ex/s avg=6,313.2 ex/s | elapsed=3.3m eta=2.0m\n",
      "[val.skip] 1,250,000/2,000,000 | inst=6,517.8 ex/s avg=6,314.7 ex/s | elapsed=3.3m eta=2.0m\n",
      "[val.skip] 1,260,000/2,000,000 | inst=8,491.3 ex/s avg=6,327.6 ex/s | elapsed=3.3m eta=1.9m\n",
      "[val.skip] 1,270,000/2,000,000 | inst=4,831.6 ex/s avg=6,312.2 ex/s | elapsed=3.4m eta=1.9m\n",
      "[val.skip] 1,280,000/2,000,000 | inst=8,055.4 ex/s avg=6,322.9 ex/s | elapsed=3.4m eta=1.9m\n",
      "[val.skip] 1,290,000/2,000,000 | inst=4,824.1 ex/s avg=6,307.7 ex/s | elapsed=3.4m eta=1.9m\n",
      "[val.skip] 1,300,000/2,000,000 | inst=6,675.7 ex/s avg=6,310.4 ex/s | elapsed=3.4m eta=1.8m\n",
      "[val.skip] 1,310,000/2,000,000 | inst=7,240.3 ex/s avg=6,316.6 ex/s | elapsed=3.5m eta=1.8m\n",
      "[val.skip] 1,320,000/2,000,000 | inst=6,975.8 ex/s avg=6,321.1 ex/s | elapsed=3.5m eta=1.8m\n",
      "[val.skip] 1,330,000/2,000,000 | inst=6,282.6 ex/s avg=6,320.8 ex/s | elapsed=3.5m eta=1.8m\n",
      "[val.skip] 1,340,000/2,000,000 | inst=7,430.3 ex/s avg=6,327.9 ex/s | elapsed=3.5m eta=1.7m\n",
      "[val.skip] 1,350,000/2,000,000 | inst=6,899.5 ex/s avg=6,331.8 ex/s | elapsed=3.6m eta=1.7m\n",
      "[val.skip] 1,360,000/2,000,000 | inst=6,849.6 ex/s avg=6,335.3 ex/s | elapsed=3.6m eta=1.7m\n",
      "[val.skip] 1,370,000/2,000,000 | inst=5,876.5 ex/s avg=6,331.7 ex/s | elapsed=3.6m eta=1.7m\n",
      "[val.skip] 1,380,000/2,000,000 | inst=6,279.3 ex/s avg=6,331.3 ex/s | elapsed=3.6m eta=1.6m\n",
      "[val.skip] 1,390,000/2,000,000 | inst=7,684.9 ex/s avg=6,339.3 ex/s | elapsed=3.7m eta=1.6m\n",
      "[val.skip] 1,400,000/2,000,000 | inst=5,963.7 ex/s avg=6,336.5 ex/s | elapsed=3.7m eta=1.6m\n",
      "[val.skip] 1,410,000/2,000,000 | inst=6,350.7 ex/s avg=6,336.6 ex/s | elapsed=3.7m eta=1.6m\n",
      "[val.skip] 1,420,000/2,000,000 | inst=1,654.7 ex/s avg=6,212.8 ex/s | elapsed=3.8m eta=1.6m\n",
      "[val.skip] 1,430,000/2,000,000 | inst=6,174.2 ex/s avg=6,212.5 ex/s | elapsed=3.8m eta=1.5m\n",
      "[val.skip] 1,440,000/2,000,000 | inst=8,000.9 ex/s avg=6,222.2 ex/s | elapsed=3.9m eta=1.5m\n",
      "[val.skip] 1,450,000/2,000,000 | inst=6,438.3 ex/s avg=6,223.6 ex/s | elapsed=3.9m eta=1.5m\n",
      "[val.skip] 1,460,000/2,000,000 | inst=7,844.4 ex/s avg=6,232.4 ex/s | elapsed=3.9m eta=1.4m\n",
      "[val.skip] 1,470,000/2,000,000 | inst=6,717.6 ex/s avg=6,235.5 ex/s | elapsed=3.9m eta=1.4m\n",
      "[val.skip] 1,480,000/2,000,000 | inst=6,196.4 ex/s avg=6,235.2 ex/s | elapsed=4.0m eta=1.4m\n",
      "[val.skip] 1,490,000/2,000,000 | inst=6,519.6 ex/s avg=6,237.1 ex/s | elapsed=4.0m eta=1.4m\n",
      "[val.skip] 1,500,000/2,000,000 | inst=6,867.7 ex/s avg=6,240.9 ex/s | elapsed=4.0m eta=1.3m\n",
      "[val.skip] 1,510,000/2,000,000 | inst=6,232.8 ex/s avg=6,240.8 ex/s | elapsed=4.0m eta=1.3m\n",
      "[val.skip] 1,520,000/2,000,000 | inst=5,481.2 ex/s avg=6,235.1 ex/s | elapsed=4.1m eta=1.3m\n",
      "[val.skip] 1,530,000/2,000,000 | inst=6,628.1 ex/s avg=6,237.6 ex/s | elapsed=4.1m eta=1.3m\n",
      "[val.skip] 1,540,000/2,000,000 | inst=6,784.5 ex/s avg=6,240.8 ex/s | elapsed=4.1m eta=1.2m\n",
      "[val.skip] 1,550,000/2,000,000 | inst=6,498.1 ex/s avg=6,242.4 ex/s | elapsed=4.1m eta=1.2m\n",
      "[val.skip] 1,560,000/2,000,000 | inst=4,701.5 ex/s avg=6,229.3 ex/s | elapsed=4.2m eta=1.2m\n",
      "[val.skip] 1,570,000/2,000,000 | inst=6,727.1 ex/s avg=6,232.3 ex/s | elapsed=4.2m eta=1.1m\n",
      "[val.skip] 1,580,000/2,000,000 | inst=5,119.9 ex/s avg=6,223.7 ex/s | elapsed=4.2m eta=1.1m\n",
      "[val.skip] 1,590,000/2,000,000 | inst=7,671.0 ex/s avg=6,231.1 ex/s | elapsed=4.3m eta=1.1m\n",
      "[val.skip] 1,600,000/2,000,000 | inst=6,121.2 ex/s avg=6,230.4 ex/s | elapsed=4.3m eta=1.1m\n",
      "[val.skip] 1,610,000/2,000,000 | inst=8,587.5 ex/s avg=6,241.0 ex/s | elapsed=4.3m eta=1.0m\n",
      "[val.skip] 1,620,000/2,000,000 | inst=4,979.8 ex/s avg=6,231.3 ex/s | elapsed=4.3m eta=1.0m\n",
      "[val.skip] 1,630,000/2,000,000 | inst=6,640.7 ex/s avg=6,233.7 ex/s | elapsed=4.4m eta=1.0m\n",
      "[val.skip] 1,640,000/2,000,000 | inst=7,846.5 ex/s avg=6,241.5 ex/s | elapsed=4.4m eta=1.0m\n",
      "[val.skip] 1,650,000/2,000,000 | inst=6,890.4 ex/s avg=6,245.1 ex/s | elapsed=4.4m eta=0.9m\n",
      "[val.skip] 1,660,000/2,000,000 | inst=5,260.0 ex/s avg=6,238.0 ex/s | elapsed=4.4m eta=0.9m\n",
      "[val.skip] 1,670,000/2,000,000 | inst=7,816.7 ex/s avg=6,245.6 ex/s | elapsed=4.5m eta=0.9m\n",
      "[val.skip] 1,680,000/2,000,000 | inst=6,562.1 ex/s avg=6,247.4 ex/s | elapsed=4.5m eta=0.9m\n",
      "[val.skip] 1,690,000/2,000,000 | inst=6,922.5 ex/s avg=6,251.0 ex/s | elapsed=4.5m eta=0.8m\n",
      "[val.skip] 1,700,000/2,000,000 | inst=4,975.7 ex/s avg=6,241.6 ex/s | elapsed=4.5m eta=0.8m\n",
      "[val.skip] 1,710,000/2,000,000 | inst=7,664.3 ex/s avg=6,248.3 ex/s | elapsed=4.6m eta=0.8m\n",
      "[val.skip] 1,720,000/2,000,000 | inst=6,302.5 ex/s avg=6,248.7 ex/s | elapsed=4.6m eta=0.7m\n",
      "[val.skip] 1,730,000/2,000,000 | inst=6,305.0 ex/s avg=6,249.0 ex/s | elapsed=4.6m eta=0.7m\n",
      "[val.skip] 1,740,000/2,000,000 | inst=8,353.4 ex/s avg=6,258.0 ex/s | elapsed=4.6m eta=0.7m\n",
      "[val.skip] 1,750,000/2,000,000 | inst=6,430.5 ex/s avg=6,259.0 ex/s | elapsed=4.7m eta=0.7m\n",
      "[val.skip] 1,760,000/2,000,000 | inst=6,204.9 ex/s avg=6,258.7 ex/s | elapsed=4.7m eta=0.6m\n",
      "[val.skip] 1,770,000/2,000,000 | inst=7,404.8 ex/s avg=6,264.2 ex/s | elapsed=4.7m eta=0.6m\n",
      "[val.skip] 1,780,000/2,000,000 | inst=7,000.4 ex/s avg=6,267.9 ex/s | elapsed=4.7m eta=0.6m\n",
      "[val.skip] 1,790,000/2,000,000 | inst=6,435.6 ex/s avg=6,268.8 ex/s | elapsed=4.8m eta=0.6m\n",
      "[val.skip] 1,800,000/2,000,000 | inst=6,994.6 ex/s avg=6,272.4 ex/s | elapsed=4.8m eta=0.5m\n",
      "[val.skip] 1,810,000/2,000,000 | inst=4,949.1 ex/s avg=6,263.1 ex/s | elapsed=4.8m eta=0.5m\n",
      "[val.skip] 1,820,000/2,000,000 | inst=7,773.8 ex/s avg=6,269.8 ex/s | elapsed=4.8m eta=0.5m\n",
      "[val.skip] 1,830,000/2,000,000 | inst=7,727.5 ex/s avg=6,276.3 ex/s | elapsed=4.9m eta=0.5m\n",
      "[val.skip] 1,840,000/2,000,000 | inst=6,503.3 ex/s avg=6,277.5 ex/s | elapsed=4.9m eta=0.4m\n",
      "[val.skip] 1,850,000/2,000,000 | inst=4,756.3 ex/s avg=6,266.7 ex/s | elapsed=4.9m eta=0.4m\n",
      "[val.skip] 1,860,000/2,000,000 | inst=5,062.2 ex/s avg=6,258.7 ex/s | elapsed=5.0m eta=0.4m\n",
      "[val.skip] 1,870,000/2,000,000 | inst=5,712.9 ex/s avg=6,255.5 ex/s | elapsed=5.0m eta=0.3m\n",
      "[val.skip] 1,880,000/2,000,000 | inst=8,019.6 ex/s avg=6,262.8 ex/s | elapsed=5.0m eta=0.3m\n",
      "[val.skip] 1,890,000/2,000,000 | inst=6,473.5 ex/s avg=6,263.9 ex/s | elapsed=5.0m eta=0.3m\n",
      "[val.skip] 1,900,000/2,000,000 | inst=5,595.1 ex/s avg=6,259.9 ex/s | elapsed=5.1m eta=0.3m\n",
      "[val.skip] 1,910,000/2,000,000 | inst=4,862.1 ex/s avg=6,250.5 ex/s | elapsed=5.1m eta=0.2m\n",
      "[val.skip] 1,920,000/2,000,000 | inst=5,395.1 ex/s avg=6,245.4 ex/s | elapsed=5.1m eta=0.2m\n",
      "[val.skip] 1,930,000/2,000,000 | inst=6,380.0 ex/s avg=6,246.0 ex/s | elapsed=5.1m eta=0.2m\n",
      "[val.skip] 1,940,000/2,000,000 | inst=8,025.7 ex/s avg=6,253.2 ex/s | elapsed=5.2m eta=0.2m\n",
      "[val.skip] 1,950,000/2,000,000 | inst=6,244.3 ex/s avg=6,253.2 ex/s | elapsed=5.2m eta=0.1m\n",
      "[val.skip] 1,960,000/2,000,000 | inst=7,034.7 ex/s avg=6,256.7 ex/s | elapsed=5.2m eta=0.1m\n",
      "[val.skip] 1,970,000/2,000,000 | inst=6,223.8 ex/s avg=6,256.5 ex/s | elapsed=5.2m eta=0.1m\n",
      "[val.skip] 1,980,000/2,000,000 | inst=8,201.1 ex/s avg=6,264.0 ex/s | elapsed=5.3m eta=0.1m\n",
      "[val.skip] 1,990,000/2,000,000 | inst=7,135.9 ex/s avg=6,267.9 ex/s | elapsed=5.3m eta=0.0m\n",
      "[val.skip] 2,000,000/2,000,000 | inst=6,001.0 ex/s avg=6,266.5 ex/s | elapsed=5.3m eta=0.0m\n",
      "[build_streaming_splits] val stream ready in 320.17s\n",
      "=== BUILDING TOKEN STREAMS ===\n",
      "=== BUILDING WINDOW STREAMS ===\n",
      "=== BUILDING BATCH STREAMS ===\n",
      "=== SANITY CHECK: TRAIN ===\n",
      "[batch_stream] batch=1 batch_time=6.02s avg_batch_time=6.02s\n",
      "[sanity] train batch fetched in 6.02s\n",
      "train batch sanity: {'B': 8, 'T': 1024, 'xb_range': (1, 50013), 'yb_range': (1, 50013)} fp 82a5aacedc\n",
      "=== SANITY CHECK: VAL ===\n",
      "[batch_stream] batch=1 batch_time=0.01s avg_batch_time=0.01s\n",
      "[sanity] val batch fetched in 0.01s\n",
      "val batch sanity: {'B': 8, 'T': 1024, 'xb_range': (0, 50232), 'yb_range': (0, 50232)} fp fa0dc56ed6\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, hashlib, time\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "\n",
    "# ---------------- SPLIT CONFIG ----------------\n",
    "SEED = 1337\n",
    "TRAIN_EXAMPLES = 2_000_000\n",
    "VAL_EXAMPLES   = 10_000\n",
    "SHUFFLE_BUFFER = 50_000\n",
    "# -------------- STREAM CONFIG -----------------\n",
    "STRIDE = context_len\n",
    "MAX_BUFFER_TOKENS = 2_000_000\n",
    "# ------------------------------------------------\n",
    "\n",
    "def _drain_with_progress(it, n, label=\"[skip]\"):\n",
    "    \"\"\"\n",
    "    Drain exactly n items from iterator it, printing progress + ETA.\n",
    "    Returns the SAME iterator positioned after draining.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    last_t = t0\n",
    "    last_i = 0\n",
    "\n",
    "    # We can’t “seek” in streaming; we must consume.\n",
    "    for i in range(1, n + 1):\n",
    "        try:\n",
    "            next(it)\n",
    "        except StopIteration:\n",
    "            print(f\"{label} iterator ended early at {i-1}/{n}\")\n",
    "            return it\n",
    "\n",
    "        # Print often at start, then every 10k\n",
    "        if i <= 10 or i % 10_000 == 0:\n",
    "            now = time.time()\n",
    "            elapsed = now - t0\n",
    "            dt = now - last_t\n",
    "            di = i - last_i\n",
    "            rate = (di / dt) if dt > 0 else 0.0\n",
    "\n",
    "            # overall avg rate + ETA\n",
    "            avg_rate = (i / elapsed) if elapsed > 0 else 0.0\n",
    "            remaining = n - i\n",
    "            eta = (remaining / avg_rate) if avg_rate > 0 else float(\"inf\")\n",
    "\n",
    "            print(\n",
    "                f\"{label} {i:,}/{n:,} \"\n",
    "                f\"| inst={rate:,.1f} ex/s avg={avg_rate:,.1f} ex/s \"\n",
    "                f\"| elapsed={elapsed/60:.1f}m eta={eta/60:.1f}m\"\n",
    "            )\n",
    "\n",
    "            last_t = now\n",
    "            last_i = i\n",
    "\n",
    "    return it\n",
    "\n",
    "def build_streaming_splits(dataset_name=\"monology/pile-uncopyrighted\",\n",
    "                           split=\"train\",\n",
    "                           train_examples=TRAIN_EXAMPLES,\n",
    "                           val_examples=VAL_EXAMPLES,\n",
    "                           seed=SEED,\n",
    "                           shuffle_buffer=SHUFFLE_BUFFER):\n",
    "    print(\"[build_streaming_splits] loading dataset...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    ds = load_dataset(dataset_name, split=split, streaming=True)\n",
    "\n",
    "    print(f\"[build_streaming_splits] dataset object ready in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # IMPORTANT:\n",
    "    # We MUST create two independent streaming iterators if we want to both:\n",
    "    # - train on first N (shuffled)\n",
    "    # - val on the next M after skipping N\n",
    "    # But streaming datasets are single-pass iterables; calling .take() and .skip()\n",
    "    # on the same ds object can still be okay lazily, but progress during skip is invisible.\n",
    "    #\n",
    "    # Here: we explicitly build train stream from ds, and build val stream by creating\n",
    "    # a fresh streaming dataset iterator, draining N with progress, then taking M.\n",
    "\n",
    "    print(\"[build_streaming_splits] building train stream...\")\n",
    "    t1 = time.time()\n",
    "    train_ds = ds.take(train_examples).shuffle(seed=seed, buffer_size=shuffle_buffer)\n",
    "    print(f\"[build_streaming_splits] train stream ready in {time.time()-t1:.2f}s\")\n",
    "\n",
    "    print(\"[build_streaming_splits] building val stream (skip with progress)...\")\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Fresh iterator for val path (so we don't consume the train path)\n",
    "    ds_val_base = load_dataset(dataset_name, split=split, streaming=True)\n",
    "    it = iter(ds_val_base)\n",
    "\n",
    "    # Drain exactly TRAIN_EXAMPLES with visible progress\n",
    "    it = _drain_with_progress(it, train_examples, label=\"[val.skip]\")\n",
    "\n",
    "    # Now take val_examples from remaining stream\n",
    "    # Wrap back into a simple iterable of dict examples\n",
    "    def _val_iter():\n",
    "        for ex in islice(it, val_examples):\n",
    "            yield ex\n",
    "\n",
    "    val_ds = _val_iter()\n",
    "\n",
    "    print(f\"[build_streaming_splits] val stream ready in {time.time()-t2:.2f}s\")\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def token_stream(hf_dataset, tokenizer):\n",
    "    doc_count = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ex in hf_dataset:\n",
    "        doc_count += 1\n",
    "        if doc_count % 10_000 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"[token_stream] {doc_count} docs in {elapsed:.1f}s ({doc_count/elapsed:.1f} docs/s)\")\n",
    "\n",
    "        text = ex.get(\"text\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        ids = tokenizer.encode(text)\n",
    "        if len(ids) >= 2:\n",
    "            yield ids\n",
    "\n",
    "def window_stream(token_iter, context_len, *, device, stride=STRIDE, max_buffer_tokens=MAX_BUFFER_TOKENS):\n",
    "    buf = deque()\n",
    "    total_tokens = 0\n",
    "    yielded = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ids in token_iter:\n",
    "        buf.extend(ids)\n",
    "        total_tokens += len(ids)\n",
    "\n",
    "        if total_tokens % 100_000 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            print(\n",
    "                f\"[window_stream] tokens buffered={len(buf)} \"\n",
    "                f\"total_seen={total_tokens} \"\n",
    "                f\"rate={total_tokens/elapsed:.0f} tok/s \"\n",
    "                f\"samples_yielded={yielded}\"\n",
    "            )\n",
    "\n",
    "        while len(buf) > max_buffer_tokens:\n",
    "            buf.popleft()\n",
    "\n",
    "        while len(buf) >= context_len + 1:\n",
    "            x = [buf[i] for i in range(context_len)]\n",
    "            y = [buf[i+1] for i in range(context_len)]\n",
    "\n",
    "            for _ in range(stride):\n",
    "                if not buf:\n",
    "                    break\n",
    "                buf.popleft()\n",
    "\n",
    "            yielded += 1\n",
    "            if yielded % 100 == 0:\n",
    "                elapsed = time.time() - t0\n",
    "                print(\n",
    "                    f\"[window_stream] yielded={yielded} \"\n",
    "                    f\"avg_time_per_sample={elapsed/yielded:.3f}s\"\n",
    "                )\n",
    "\n",
    "            yield (\n",
    "                torch.tensor(x, dtype=torch.long, device=device),\n",
    "                torch.tensor(y, dtype=torch.long, device=device),\n",
    "            )\n",
    "\n",
    "def batch_stream(sample_iter, batch_size):\n",
    "    batch_count = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    while True:\n",
    "        batch_count += 1\n",
    "        tb0 = time.time()\n",
    "        xb, yb = zip(*(next(sample_iter) for _ in range(batch_size)))\n",
    "        tb1 = time.time()\n",
    "\n",
    "        if batch_count <= 5 or batch_count % 10 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            print(\n",
    "                f\"[batch_stream] batch={batch_count} \"\n",
    "                f\"batch_time={tb1-tb0:.2f}s \"\n",
    "                f\"avg_batch_time={elapsed/batch_count:.2f}s\"\n",
    "            )\n",
    "\n",
    "        yield torch.stack(xb, 0), torch.stack(yb, 0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sanity_check_batch(xb, yb, vocab_size, context_len):\n",
    "    assert xb.ndim == 2 and yb.ndim == 2\n",
    "    B, T = xb.shape\n",
    "    assert yb.shape == (B, T)\n",
    "    assert T == context_len\n",
    "\n",
    "    x_min, x_max = int(xb.min()), int(xb.max())\n",
    "    y_min, y_max = int(yb.min()), int(yb.max())\n",
    "    assert 0 <= x_min and x_max < vocab_size\n",
    "    assert 0 <= y_min and y_max < vocab_size\n",
    "\n",
    "    assert torch.equal(yb[:, :-1], xb[:, 1:])\n",
    "    assert not torch.equal(xb, yb)\n",
    "\n",
    "    return {\"B\": B, \"T\": T, \"xb_range\": (x_min, x_max), \"yb_range\": (y_min, y_max)}\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_from_iters(model, train_batch_iter, val_batch_iter, eval_batches=20, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for name, it in [(\"train\", train_batch_iter), (\"val\", val_batch_iter)]:\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        for i in range(eval_batches):\n",
    "            xb, yb = next(it)\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[estimate_loss] {name} {eval_batches} batches in {elapsed:.2f}s\")\n",
    "        out[name] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def batch_fingerprint(xb):\n",
    "    b = xb[0, :16].detach().cpu().numpy().tobytes()\n",
    "    return hashlib.sha1(b).hexdigest()[:10]\n",
    "\n",
    "# ---------------- BUILD ----------------\n",
    "print(\"=== BUILDING STREAMS ===\")\n",
    "train_ds, val_ds = build_streaming_splits()\n",
    "\n",
    "print(\"=== BUILDING TOKEN STREAMS ===\")\n",
    "train_tok = token_stream(train_ds, tokenizer)\n",
    "val_tok   = token_stream(val_ds, tokenizer)\n",
    "\n",
    "print(\"=== BUILDING WINDOW STREAMS ===\")\n",
    "train_samples = window_stream(train_tok, context_len, device=device)\n",
    "val_samples   = window_stream(val_tok,   context_len, device=device)\n",
    "\n",
    "print(\"=== BUILDING BATCH STREAMS ===\")\n",
    "train_batch_iter = batch_stream(train_samples, batch_size)\n",
    "val_batch_iter   = batch_stream(val_samples,   batch_size)\n",
    "\n",
    "# ---------------- SANITY ----------------\n",
    "print(\"=== SANITY CHECK: TRAIN ===\")\n",
    "t0 = time.time()\n",
    "xb, yb = next(train_batch_iter)\n",
    "print(f\"[sanity] train batch fetched in {time.time()-t0:.2f}s\")\n",
    "print(\"train batch sanity:\", sanity_check_batch(xb, yb, vocab_size, context_len), \"fp\", batch_fingerprint(xb))\n",
    "\n",
    "print(\"=== SANITY CHECK: VAL ===\")\n",
    "t1 = time.time()\n",
    "xb, yb = next(val_batch_iter)\n",
    "print(f\"[sanity] val batch fetched in {time.time()-t1:.2f}s\")\n",
    "print(\"val batch sanity:\", sanity_check_batch(xb, yb, vocab_size, context_len), \"fp\", batch_fingerprint(xb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YxLoyfRq9Lh7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe45db77323e450bae7af6ece6e3caa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch_stream] batch=240 batch_time=0.01s avg_batch_time=0.56s\n",
      "fp ef30f40788\n",
      "step     0 | lr 3.000e-06 | train_loss 10.0673\n",
      "[window_stream] yielded=2000 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=250 batch_time=0.01s avg_batch_time=0.56s\n",
      "step    10 | lr 3.000e-06 | train_loss 9.1881\n",
      "[batch_stream] batch=260 batch_time=0.01s avg_batch_time=0.55s\n",
      "step    20 | lr 3.000e-06 | train_loss 9.1061\n",
      "[window_stream] yielded=2100 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=270 batch_time=0.00s avg_batch_time=0.54s\n",
      "step    30 | lr 3.000e-06 | train_loss 9.5590\n",
      "[window_stream] yielded=2200 avg_time_per_sample=0.068s\n",
      "[batch_stream] batch=280 batch_time=0.00s avg_batch_time=0.54s\n",
      "step    40 | lr 3.000e-06 | train_loss 8.3156\n",
      "[window_stream] yielded=2300 avg_time_per_sample=0.067s\n",
      "[batch_stream] batch=290 batch_time=0.00s avg_batch_time=0.54s\n",
      "step    50 | lr 3.000e-06 | train_loss 6.9631\n",
      "[window_stream] yielded=2400 avg_time_per_sample=0.066s\n",
      "[batch_stream] batch=300 batch_time=0.01s avg_batch_time=0.53s\n",
      "step    60 | lr 3.000e-06 | train_loss 9.3687\n",
      "[batch_stream] batch=310 batch_time=0.00s avg_batch_time=0.53s\n",
      "step    70 | lr 3.000e-06 | train_loss 9.0841\n",
      "[window_stream] yielded=2500 avg_time_per_sample=0.066s\n",
      "[batch_stream] batch=320 batch_time=0.00s avg_batch_time=0.52s\n",
      "step    80 | lr 3.000e-06 | train_loss 8.6628\n",
      "[window_stream] yielded=2600 avg_time_per_sample=0.065s\n",
      "[batch_stream] batch=330 batch_time=0.00s avg_batch_time=0.52s\n",
      "step    90 | lr 3.000e-06 | train_loss 8.9045\n",
      "[window_stream] yielded=2700 avg_time_per_sample=0.065s\n",
      "[batch_stream] batch=340 batch_time=0.00s avg_batch_time=0.52s\n",
      "step   100 | lr 3.000e-06 | train_loss 7.6931\n",
      "[window_stream] yielded=2800 avg_time_per_sample=0.064s\n",
      "[batch_stream] batch=350 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   110 | lr 3.000e-06 | train_loss 8.6725\n",
      "[batch_stream] batch=360 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   120 | lr 3.000e-06 | train_loss 7.5726\n",
      "[window_stream] yielded=2900 avg_time_per_sample=0.064s\n",
      "[batch_stream] batch=370 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   130 | lr 3.000e-06 | train_loss 8.3957\n",
      "[window_stream] yielded=3000 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=380 batch_time=0.02s avg_batch_time=0.51s\n",
      "step   140 | lr 3.000e-06 | train_loss 10.6597\n",
      "[window_stream] yielded=3100 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=390 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   150 | lr 3.000e-06 | train_loss 9.3210\n",
      "[window_stream] yielded=3200 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=400 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   160 | lr 3.000e-06 | train_loss 8.7644\n",
      "[batch_stream] batch=410 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   170 | lr 3.000e-06 | train_loss 7.9231\n",
      "[window_stream] yielded=3300 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=420 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   180 | lr 3.000e-06 | train_loss 6.4317\n",
      "[window_stream] yielded=3400 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=430 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   190 | lr 3.000e-06 | train_loss 7.6713\n",
      "[window_stream] yielded=3500 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=440 batch_time=0.01s avg_batch_time=0.49s\n",
      "step   200 | lr 3.000e-06 | train_loss 8.1291\n",
      "[window_stream] yielded=3600 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=450 batch_time=0.00s avg_batch_time=0.50s\n",
      "[estimate_loss] train 10 batches in 8.93s\n",
      "[window_stream] yielded=100 avg_time_per_sample=2.213s\n",
      "[batch_stream] batch=20 batch_time=0.00s avg_batch_time=11.38s\n",
      "[estimate_loss] val 10 batches in 8.96s\n",
      "[eval] step   200 | train 2.9485 | val 7.7877\n",
      "[batch_stream] batch=460 batch_time=0.01s avg_batch_time=0.52s\n",
      "step   210 | lr 3.000e-06 | train_loss 8.2073\n",
      "[window_stream] yielded=3700 avg_time_per_sample=0.065s\n",
      "[batch_stream] batch=470 batch_time=0.01s avg_batch_time=0.52s\n",
      "step   220 | lr 3.000e-06 | train_loss 7.2481\n",
      "[window_stream] yielded=3800 avg_time_per_sample=0.065s\n",
      "[batch_stream] batch=480 batch_time=0.03s avg_batch_time=0.52s\n",
      "step   230 | lr 3.000e-06 | train_loss 7.8473\n",
      "[window_stream] yielded=3900 avg_time_per_sample=0.064s\n",
      "[batch_stream] batch=490 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   240 | lr 3.000e-06 | train_loss 7.5102\n",
      "[window_stream] yielded=4000 avg_time_per_sample=0.064s\n",
      "[batch_stream] batch=500 batch_time=0.01s avg_batch_time=0.51s\n",
      "step   250 | lr 3.000e-06 | train_loss 7.8136\n",
      "[batch_stream] batch=510 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   260 | lr 3.000e-06 | train_loss 7.0848\n",
      "[window_stream] yielded=4100 avg_time_per_sample=0.064s\n",
      "[batch_stream] batch=520 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   270 | lr 3.000e-06 | train_loss 8.2510\n",
      "[window_stream] yielded=4200 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=530 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   280 | lr 3.000e-06 | train_loss 7.8235\n",
      "[window_stream] yielded=4300 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=540 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   290 | lr 3.000e-06 | train_loss 5.8896\n",
      "[window_stream] yielded=4400 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=550 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   300 | lr 3.000e-06 | train_loss 7.9156\n",
      "[batch_stream] batch=560 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   310 | lr 3.000e-06 | train_loss 7.6909\n",
      "[window_stream] yielded=4500 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=570 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   320 | lr 3.000e-06 | train_loss 7.8422\n",
      "[window_stream] yielded=4600 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=580 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   330 | lr 3.000e-06 | train_loss 7.9181\n",
      "[window_stream] yielded=4700 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=590 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   340 | lr 3.000e-06 | train_loss 7.7049\n",
      "[window_stream] yielded=4800 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=600 batch_time=0.01s avg_batch_time=0.49s\n",
      "step   350 | lr 3.000e-06 | train_loss 7.5523\n",
      "[batch_stream] batch=610 batch_time=0.01s avg_batch_time=0.49s\n",
      "step   360 | lr 3.000e-06 | train_loss 7.2034\n",
      "[window_stream] yielded=4900 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=620 batch_time=0.00s avg_batch_time=0.49s\n",
      "step   370 | lr 3.000e-06 | train_loss 6.7710\n",
      "[window_stream] yielded=5000 avg_time_per_sample=0.061s\n",
      "[batch_stream] batch=630 batch_time=0.00s avg_batch_time=0.49s\n",
      "step   380 | lr 3.000e-06 | train_loss 5.4998\n",
      "[window_stream] yielded=5100 avg_time_per_sample=0.061s\n",
      "[batch_stream] batch=640 batch_time=0.00s avg_batch_time=0.49s\n",
      "step   390 | lr 3.000e-06 | train_loss 8.0436\n",
      "[window_stream] yielded=5200 avg_time_per_sample=0.061s\n",
      "[batch_stream] batch=650 batch_time=0.01s avg_batch_time=0.49s\n",
      "step   400 | lr 3.000e-06 | train_loss 7.1529\n",
      "[batch_stream] batch=660 batch_time=0.01s avg_batch_time=0.49s\n",
      "[estimate_loss] train 10 batches in 8.95s\n",
      "[window_stream] yielded=200 avg_time_per_sample=1.618s\n",
      "[batch_stream] batch=30 batch_time=0.00s avg_batch_time=10.93s\n",
      "[estimate_loss] val 10 batches in 8.95s\n",
      "[eval] step   400 | train 7.4625 | val 7.6011\n",
      "[window_stream] yielded=5300 avg_time_per_sample=0.064s\n",
      "[batch_stream] batch=670 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   410 | lr 3.000e-06 | train_loss 3.5927\n",
      "[window_stream] yielded=5400 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=680 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   420 | lr 3.000e-06 | train_loss 6.9535\n",
      "[window_stream] yielded=5500 avg_time_per_sample=0.064s\n",
      "[batch_stream] batch=690 batch_time=0.01s avg_batch_time=0.51s\n",
      "step   430 | lr 3.000e-06 | train_loss 6.8660\n",
      "[window_stream] yielded=5600 avg_time_per_sample=0.064s\n",
      "[batch_stream] batch=700 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   440 | lr 3.000e-06 | train_loss 8.2274\n",
      "[batch_stream] batch=710 batch_time=0.01s avg_batch_time=0.51s\n",
      "step   450 | lr 3.000e-06 | train_loss 7.7821\n",
      "[window_stream] yielded=5700 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=720 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   460 | lr 3.000e-06 | train_loss 7.4323\n",
      "[window_stream] yielded=5800 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=730 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   470 | lr 3.000e-06 | train_loss 7.1299\n",
      "[window_stream] yielded=5900 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=740 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   480 | lr 3.000e-06 | train_loss 6.8584\n",
      "[window_stream] yielded=6000 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=750 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   490 | lr 3.000e-06 | train_loss 7.1389\n",
      "[batch_stream] batch=760 batch_time=0.00s avg_batch_time=0.50s\n",
      "fp e841e6eb59\n",
      "step   500 | lr 3.000e-06 | train_loss 5.3281\n",
      "[window_stream] yielded=6100 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=770 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   510 | lr 3.000e-06 | train_loss 6.3284\n",
      "[window_stream] yielded=6200 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=780 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   520 | lr 3.000e-06 | train_loss 7.7794\n",
      "[window_stream] yielded=6300 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=790 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   530 | lr 3.000e-06 | train_loss 7.2734\n",
      "[window_stream] yielded=6400 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=800 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   540 | lr 3.000e-06 | train_loss 7.7357\n",
      "[batch_stream] batch=810 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   550 | lr 3.000e-06 | train_loss 6.5531\n",
      "[window_stream] yielded=6500 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=820 batch_time=0.01s avg_batch_time=0.49s\n",
      "step   560 | lr 3.000e-06 | train_loss 7.4786\n",
      "[window_stream] yielded=6600 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=830 batch_time=0.00s avg_batch_time=0.49s\n",
      "step   570 | lr 3.000e-06 | train_loss 5.9735\n",
      "[window_stream] yielded=6700 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=840 batch_time=0.00s avg_batch_time=0.49s\n",
      "step   580 | lr 3.000e-06 | train_loss 7.9954\n",
      "[window_stream] yielded=6800 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=850 batch_time=0.00s avg_batch_time=0.49s\n",
      "step   590 | lr 3.000e-06 | train_loss 7.8872\n",
      "[batch_stream] batch=860 batch_time=0.01s avg_batch_time=0.49s\n",
      "step   600 | lr 3.000e-06 | train_loss 7.2462\n",
      "[window_stream] yielded=6900 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=870 batch_time=0.00s avg_batch_time=0.50s\n",
      "[estimate_loss] train 10 batches in 8.97s\n",
      "[window_stream] yielded=300 avg_time_per_sample=1.437s\n",
      "[batch_stream] batch=40 batch_time=0.01s avg_batch_time=10.82s\n",
      "[estimate_loss] val 10 batches in 8.96s\n",
      "[eval] step   600 | train 6.7871 | val 6.6764\n",
      "[window_stream] yielded=7000 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=880 batch_time=0.00s avg_batch_time=0.51s\n",
      "step   610 | lr 3.000e-06 | train_loss 7.5938\n",
      "[window_stream] yielded=7100 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=890 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   620 | lr 3.000e-06 | train_loss 7.3544\n",
      "[window_stream] yielded=7200 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=900 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   630 | lr 3.000e-06 | train_loss 7.4146\n",
      "[batch_stream] batch=910 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   640 | lr 3.000e-06 | train_loss 6.9428\n",
      "[window_stream] yielded=7300 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=920 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   650 | lr 3.000e-06 | train_loss 8.2770\n",
      "[window_stream] yielded=7400 avg_time_per_sample=0.063s\n",
      "[batch_stream] batch=930 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   660 | lr 3.000e-06 | train_loss 4.4988\n",
      "[window_stream] yielded=7500 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=940 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   670 | lr 3.000e-06 | train_loss 7.3320\n",
      "[window_stream] yielded=7600 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=950 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   680 | lr 3.000e-06 | train_loss 7.2603\n",
      "[batch_stream] batch=960 batch_time=0.01s avg_batch_time=0.50s\n",
      "step   690 | lr 3.000e-06 | train_loss 7.1568\n",
      "[window_stream] yielded=7700 avg_time_per_sample=0.062s\n",
      "[batch_stream] batch=970 batch_time=0.00s avg_batch_time=0.50s\n",
      "step   700 | lr 3.000e-06 | train_loss 6.9961\n",
      "[saved FP16 weights → weights/model_step_700.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c589601fa84913bba5a593f8779276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9236bc6e839425d8433078fb74fd8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fa2be07e17427aa7217631e54e63e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d886ef197b449982e35902ecc609ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded → hf://345rf4gt56t4r3e3/nnn/model_step_700.pt]\n",
      "[window_stream] yielded=7800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=980 batch_time=0.01s avg_batch_time=0.60s\n",
      "step   710 | lr 3.000e-06 | train_loss 6.9647\n",
      "[window_stream] yielded=7900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=990 batch_time=0.00s avg_batch_time=0.60s\n",
      "step   720 | lr 3.000e-06 | train_loss 5.8218\n",
      "[window_stream] yielded=8000 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1000 batch_time=0.01s avg_batch_time=0.59s\n",
      "step   730 | lr 3.000e-06 | train_loss 5.8928\n",
      "[batch_stream] batch=1010 batch_time=0.00s avg_batch_time=0.59s\n",
      "step   740 | lr 3.000e-06 | train_loss 6.1133\n",
      "[window_stream] yielded=8100 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1020 batch_time=0.01s avg_batch_time=0.59s\n",
      "step   750 | lr 3.000e-06 | train_loss 7.4464\n",
      "[window_stream] yielded=8200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1030 batch_time=0.02s avg_batch_time=0.59s\n",
      "step   760 | lr 3.000e-06 | train_loss 7.1951\n",
      "[window_stream] yielded=8300 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1040 batch_time=0.01s avg_batch_time=0.59s\n",
      "step   770 | lr 3.000e-06 | train_loss 7.1065\n",
      "[window_stream] yielded=8400 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=1050 batch_time=0.01s avg_batch_time=0.59s\n",
      "step   780 | lr 3.000e-06 | train_loss 7.3843\n",
      "[batch_stream] batch=1060 batch_time=0.00s avg_batch_time=0.58s\n",
      "step   790 | lr 3.000e-06 | train_loss 7.1881\n",
      "[window_stream] yielded=8500 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=1070 batch_time=0.00s avg_batch_time=0.58s\n",
      "step   800 | lr 3.000e-06 | train_loss 6.9492\n",
      "[window_stream] yielded=8600 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=1080 batch_time=0.01s avg_batch_time=0.59s\n",
      "[estimate_loss] train 10 batches in 8.94s\n",
      "[window_stream] yielded=400 avg_time_per_sample=1.585s\n",
      "[batch_stream] batch=50 batch_time=0.01s avg_batch_time=12.68s\n",
      "[estimate_loss] val 10 batches in 8.92s\n",
      "[eval] step   800 | train 6.4389 | val 6.7376\n",
      "[window_stream] yielded=8700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1090 batch_time=0.01s avg_batch_time=0.59s\n",
      "step   810 | lr 3.000e-06 | train_loss 7.4264\n",
      "[window_stream] yielded=8800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1100 batch_time=0.00s avg_batch_time=0.59s\n",
      "step   820 | lr 3.000e-06 | train_loss 3.7046\n",
      "[batch_stream] batch=1110 batch_time=0.00s avg_batch_time=0.59s\n",
      "step   830 | lr 3.000e-06 | train_loss 7.1556\n",
      "[window_stream] yielded=8900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1120 batch_time=0.00s avg_batch_time=0.59s\n",
      "step   840 | lr 3.000e-06 | train_loss 5.2707\n",
      "[window_stream] yielded=9000 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=1130 batch_time=0.01s avg_batch_time=0.59s\n",
      "step   850 | lr 3.000e-06 | train_loss 5.1836\n",
      "[window_stream] yielded=9100 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=1140 batch_time=0.01s avg_batch_time=0.58s\n",
      "step   860 | lr 3.000e-06 | train_loss 6.9712\n",
      "[window_stream] yielded=9200 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=1150 batch_time=0.01s avg_batch_time=0.58s\n",
      "step   870 | lr 3.000e-06 | train_loss 6.3221\n",
      "[batch_stream] batch=1160 batch_time=0.00s avg_batch_time=0.58s\n",
      "step   880 | lr 3.000e-06 | train_loss 6.3351\n",
      "[window_stream] yielded=9300 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=1170 batch_time=0.00s avg_batch_time=0.58s\n",
      "step   890 | lr 3.000e-06 | train_loss 3.1630\n",
      "[window_stream] yielded=9400 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=1180 batch_time=0.00s avg_batch_time=0.58s\n",
      "step   900 | lr 3.000e-06 | train_loss 7.2525\n",
      "[window_stream] yielded=9500 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=1190 batch_time=0.00s avg_batch_time=0.58s\n",
      "step   910 | lr 3.000e-06 | train_loss 7.3362\n",
      "[window_stream] yielded=9600 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=1200 batch_time=0.01s avg_batch_time=0.58s\n",
      "step   920 | lr 3.000e-06 | train_loss 7.3301\n",
      "[batch_stream] batch=1210 batch_time=0.01s avg_batch_time=0.57s\n",
      "step   930 | lr 3.000e-06 | train_loss 6.0578\n",
      "[window_stream] yielded=9700 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=1220 batch_time=0.00s avg_batch_time=0.57s\n",
      "step   940 | lr 3.000e-06 | train_loss 6.0553\n",
      "[window_stream] yielded=9800 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=1230 batch_time=0.00s avg_batch_time=0.57s\n",
      "step   950 | lr 3.000e-06 | train_loss 7.6255\n",
      "[window_stream] yielded=9900 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1240 batch_time=0.00s avg_batch_time=0.57s\n",
      "step   960 | lr 3.000e-06 | train_loss 6.7101\n",
      "[window_stream] yielded=10000 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1250 batch_time=0.01s avg_batch_time=0.57s\n",
      "step   970 | lr 3.000e-06 | train_loss 7.1918\n",
      "[batch_stream] batch=1260 batch_time=0.01s avg_batch_time=0.57s\n",
      "step   980 | lr 3.000e-06 | train_loss 7.2533\n",
      "[window_stream] yielded=10100 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1270 batch_time=0.00s avg_batch_time=0.57s\n",
      "step   990 | lr 3.000e-06 | train_loss 6.8070\n",
      "[window_stream] yielded=10200 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1280 batch_time=0.01s avg_batch_time=0.57s\n",
      "fp 59603c1a21\n",
      "step  1000 | lr 3.000e-06 | train_loss 7.1784\n",
      "[window_stream] yielded=10300 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1290 batch_time=0.00s avg_batch_time=0.57s\n",
      "[estimate_loss] train 10 batches in 8.91s\n",
      "[batch_stream] batch=60 batch_time=0.01s avg_batch_time=12.24s\n",
      "[estimate_loss] val 10 batches in 8.92s\n",
      "[eval] step  1000 | train 6.9610 | val 7.1063\n",
      "[window_stream] yielded=10400 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=1300 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1010 | lr 3.000e-06 | train_loss 4.8197\n",
      "[batch_stream] batch=1310 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  1020 | lr 3.000e-06 | train_loss 6.9717\n",
      "[window_stream] yielded=10500 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=1320 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  1030 | lr 3.000e-06 | train_loss 6.2218\n",
      "[window_stream] yielded=10600 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1330 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1040 | lr 3.000e-06 | train_loss 6.7491\n",
      "[window_stream] yielded=10700 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1340 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1050 | lr 3.000e-06 | train_loss 3.5214\n",
      "[window_stream] yielded=10800 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1350 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  1060 | lr 3.000e-06 | train_loss 7.0656\n",
      "[batch_stream] batch=1360 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1070 | lr 3.000e-06 | train_loss 5.3165\n",
      "[window_stream] yielded=10900 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1370 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1080 | lr 3.000e-06 | train_loss 6.8387\n",
      "[window_stream] yielded=11000 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=1380 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1090 | lr 3.000e-06 | train_loss 6.8778\n",
      "[window_stream] yielded=11100 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1390 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1100 | lr 3.000e-06 | train_loss 7.3265\n",
      "[window_stream] yielded=11200 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1400 batch_time=0.01s avg_batch_time=0.56s\n",
      "step  1110 | lr 3.000e-06 | train_loss 7.7001\n",
      "[batch_stream] batch=1410 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1120 | lr 3.000e-06 | train_loss 5.1817\n",
      "[window_stream] yielded=11300 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1420 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1130 | lr 3.000e-06 | train_loss 6.2061\n",
      "[window_stream] yielded=11400 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1430 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1140 | lr 3.000e-06 | train_loss 6.2742\n",
      "[window_stream] yielded=11500 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1440 batch_time=0.01s avg_batch_time=0.56s\n",
      "step  1150 | lr 3.000e-06 | train_loss 7.6758\n",
      "[window_stream] yielded=11600 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1450 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1160 | lr 3.000e-06 | train_loss 6.5778\n",
      "[batch_stream] batch=1460 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1170 | lr 3.000e-06 | train_loss 6.4773\n",
      "[window_stream] yielded=11700 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1470 batch_time=0.01s avg_batch_time=0.56s\n",
      "step  1180 | lr 3.000e-06 | train_loss 7.5362\n",
      "[window_stream] yielded=11800 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1480 batch_time=0.00s avg_batch_time=0.55s\n",
      "step  1190 | lr 3.000e-06 | train_loss 6.6880\n",
      "[window_stream] yielded=11900 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1490 batch_time=0.00s avg_batch_time=0.55s\n",
      "step  1200 | lr 3.000e-06 | train_loss 6.1915\n",
      "[window_stream] yielded=12000 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1500 batch_time=0.00s avg_batch_time=0.56s\n",
      "[estimate_loss] train 10 batches in 8.93s\n",
      "[window_stream] yielded=500 avg_time_per_sample=1.657s\n",
      "[batch_stream] batch=70 batch_time=0.00s avg_batch_time=11.93s\n",
      "[estimate_loss] val 10 batches in 8.97s\n",
      "[eval] step  1200 | train 6.0798 | val 8.4336\n",
      "[batch_stream] batch=1510 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1210 | lr 3.000e-06 | train_loss 5.7847\n",
      "[window_stream] yielded=12100 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1520 batch_time=0.01s avg_batch_time=0.56s\n",
      "step  1220 | lr 3.000e-06 | train_loss 5.3816\n",
      "[window_stream] yielded=12200 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1530 batch_time=0.01s avg_batch_time=0.56s\n",
      "step  1230 | lr 3.000e-06 | train_loss 6.9066\n",
      "[window_stream] yielded=12300 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1540 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1240 | lr 3.000e-06 | train_loss 4.7055\n",
      "[window_stream] yielded=12400 avg_time_per_sample=0.070s\n",
      "[batch_stream] batch=1550 batch_time=0.01s avg_batch_time=0.56s\n",
      "step  1250 | lr 3.000e-06 | train_loss 6.2003\n",
      "[batch_stream] batch=1560 batch_time=0.00s avg_batch_time=0.56s\n",
      "step  1260 | lr 3.000e-06 | train_loss 7.0026\n",
      "[window_stream] yielded=12500 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1570 batch_time=0.00s avg_batch_time=0.55s\n",
      "step  1270 | lr 3.000e-06 | train_loss 4.9263\n",
      "[window_stream] yielded=12600 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1580 batch_time=0.01s avg_batch_time=0.55s\n",
      "step  1280 | lr 3.000e-06 | train_loss 6.0398\n",
      "[window_stream] yielded=12700 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1590 batch_time=0.01s avg_batch_time=0.55s\n",
      "step  1290 | lr 3.000e-06 | train_loss 6.4658\n",
      "[window_stream] yielded=12800 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1600 batch_time=0.01s avg_batch_time=0.55s\n",
      "step  1300 | lr 3.000e-06 | train_loss 5.9288\n",
      "[batch_stream] batch=1610 batch_time=0.00s avg_batch_time=0.55s\n",
      "step  1310 | lr 3.000e-06 | train_loss 4.9212\n",
      "[window_stream] yielded=12900 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1620 batch_time=0.01s avg_batch_time=0.55s\n",
      "step  1320 | lr 3.000e-06 | train_loss 7.3792\n",
      "[window_stream] yielded=13000 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1630 batch_time=0.02s avg_batch_time=0.55s\n",
      "step  1330 | lr 3.000e-06 | train_loss 4.9510\n",
      "[window_stream] yielded=13100 avg_time_per_sample=0.069s\n",
      "[batch_stream] batch=1640 batch_time=0.00s avg_batch_time=0.55s\n",
      "step  1340 | lr 3.000e-06 | train_loss 3.0179\n",
      "[window_stream] yielded=13200 avg_time_per_sample=0.068s\n",
      "[batch_stream] batch=1650 batch_time=0.00s avg_batch_time=0.55s\n",
      "step  1350 | lr 3.000e-06 | train_loss 6.6506\n",
      "[batch_stream] batch=1660 batch_time=0.00s avg_batch_time=0.55s\n",
      "step  1360 | lr 3.000e-06 | train_loss 6.0496\n",
      "[window_stream] yielded=13300 avg_time_per_sample=0.068s\n",
      "[batch_stream] batch=1670 batch_time=0.01s avg_batch_time=0.55s\n",
      "step  1370 | lr 3.000e-06 | train_loss 5.6401\n",
      "[window_stream] yielded=13400 avg_time_per_sample=0.068s\n",
      "[batch_stream] batch=1680 batch_time=0.01s avg_batch_time=0.55s\n",
      "step  1380 | lr 3.000e-06 | train_loss 6.9045\n",
      "[window_stream] yielded=13500 avg_time_per_sample=0.068s\n",
      "[batch_stream] batch=1690 batch_time=0.00s avg_batch_time=0.54s\n",
      "step  1390 | lr 3.000e-06 | train_loss 6.4014\n",
      "[window_stream] yielded=13600 avg_time_per_sample=0.068s\n",
      "[batch_stream] batch=1700 batch_time=0.00s avg_batch_time=0.54s\n",
      "step  1400 | lr 3.000e-06 | train_loss 5.5609\n",
      "[batch_stream] batch=1710 batch_time=0.00s avg_batch_time=0.55s\n",
      "[estimate_loss] train 10 batches in 8.92s\n",
      "[window_stream] yielded=600 avg_time_per_sample=1.551s\n",
      "[batch_stream] batch=80 batch_time=0.00s avg_batch_time=11.69s\n",
      "[estimate_loss] val 10 batches in 8.90s\n",
      "[eval] step  1400 | train 6.4213 | val 9.2289\n",
      "[saved FP16 weights → weights/model_step_1400.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2861c05c0443c4a293c8f96188ed37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c464cfd0543d45688b265ade8299bc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfca8e061d434401879df8ec3eb3b7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4272bc7b1c470382f37eec51cd6dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded → hf://345rf4gt56t4r3e3/nnn/model_step_1400.pt]\n",
      "[window_stream] yielded=13700 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=1720 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  1410 | lr 3.000e-06 | train_loss 7.1621\n",
      "[window_stream] yielded=13800 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=1730 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  1420 | lr 3.000e-06 | train_loss 6.8651\n",
      "[window_stream] yielded=13900 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=1740 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  1430 | lr 3.000e-06 | train_loss 6.9542\n",
      "[window_stream] yielded=14000 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=1750 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  1440 | lr 3.000e-06 | train_loss 6.9205\n",
      "[batch_stream] batch=1760 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  1450 | lr 3.000e-06 | train_loss 6.8813\n",
      "[window_stream] yielded=14100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=1770 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  1460 | lr 3.000e-06 | train_loss 5.3129\n",
      "[window_stream] yielded=14200 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=1780 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  1470 | lr 3.000e-06 | train_loss 7.0092\n",
      "[window_stream] yielded=14300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=1790 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  1480 | lr 3.000e-06 | train_loss 7.1072\n",
      "[window_stream] yielded=14400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=1800 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  1490 | lr 3.000e-06 | train_loss 6.2351\n",
      "[batch_stream] batch=1810 batch_time=0.00s avg_batch_time=0.60s\n",
      "fp bb2d455668\n",
      "step  1500 | lr 3.000e-06 | train_loss 6.9976\n",
      "[window_stream] yielded=14500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=1820 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  1510 | lr 3.000e-06 | train_loss 6.2171\n",
      "[window_stream] yielded=14600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=1830 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  1520 | lr 3.000e-06 | train_loss 5.2847\n",
      "[window_stream] yielded=14700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1840 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1530 | lr 3.000e-06 | train_loss 6.6731\n",
      "[window_stream] yielded=14800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1850 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1540 | lr 3.000e-06 | train_loss 5.2828\n",
      "[batch_stream] batch=1860 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  1550 | lr 3.000e-06 | train_loss 7.0836\n",
      "[window_stream] yielded=14900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1870 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1560 | lr 3.000e-06 | train_loss 6.3816\n",
      "[window_stream] yielded=15000 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1880 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  1570 | lr 3.000e-06 | train_loss 7.2337\n",
      "[token_stream] 10000 docs in 1113.2s (9.0 docs/s)\n",
      "[window_stream] yielded=15100 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1890 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1580 | lr 3.000e-06 | train_loss 6.3376\n",
      "[window_stream] yielded=15200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1900 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  1590 | lr 3.000e-06 | train_loss 7.0906\n",
      "[batch_stream] batch=1910 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1600 | lr 3.000e-06 | train_loss 5.4674\n",
      "[window_stream] yielded=15300 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1920 batch_time=0.00s avg_batch_time=0.59s\n",
      "[estimate_loss] train 10 batches in 8.95s\n",
      "[window_stream] yielded=700 avg_time_per_sample=1.617s\n",
      "[batch_stream] batch=90 batch_time=0.00s avg_batch_time=12.60s\n",
      "[estimate_loss] val 10 batches in 8.93s\n",
      "[eval] step  1600 | train 6.1296 | val 9.0409\n",
      "[window_stream] yielded=15400 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1930 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1610 | lr 3.000e-06 | train_loss 5.6276\n",
      "[window_stream] yielded=15500 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1940 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1620 | lr 3.000e-06 | train_loss 6.7052\n",
      "[window_stream] yielded=15600 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1950 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1630 | lr 3.000e-06 | train_loss 6.1055\n",
      "[batch_stream] batch=1960 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  1640 | lr 3.000e-06 | train_loss 6.8424\n",
      "[window_stream] yielded=15700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1970 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  1650 | lr 3.000e-06 | train_loss 6.9815\n",
      "[window_stream] yielded=15800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1980 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1660 | lr 3.000e-06 | train_loss 6.9376\n",
      "[window_stream] yielded=15900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=1990 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  1670 | lr 3.000e-06 | train_loss 7.0982\n",
      "[window_stream] yielded=16000 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2000 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  1680 | lr 3.000e-06 | train_loss 5.3501\n",
      "[batch_stream] batch=2010 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1690 | lr 3.000e-06 | train_loss 5.0725\n",
      "[window_stream] yielded=16100 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2020 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  1700 | lr 3.000e-06 | train_loss 6.0507\n",
      "[window_stream] yielded=16200 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2030 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1710 | lr 3.000e-06 | train_loss 7.0896\n",
      "[window_stream] yielded=16300 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2040 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1720 | lr 3.000e-06 | train_loss 5.1911\n",
      "[window_stream] yielded=16400 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2050 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1730 | lr 3.000e-06 | train_loss 7.2511\n",
      "[batch_stream] batch=2060 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1740 | lr 3.000e-06 | train_loss 6.7638\n",
      "[window_stream] yielded=16500 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2070 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1750 | lr 3.000e-06 | train_loss 6.6803\n",
      "[window_stream] yielded=16600 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2080 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1760 | lr 3.000e-06 | train_loss 6.6206\n",
      "[window_stream] yielded=16700 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2090 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1770 | lr 3.000e-06 | train_loss 6.7144\n",
      "[window_stream] yielded=16800 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2100 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1780 | lr 3.000e-06 | train_loss 5.1250\n",
      "[batch_stream] batch=2110 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1790 | lr 3.000e-06 | train_loss 6.8179\n",
      "[window_stream] yielded=16900 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2120 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1800 | lr 3.000e-06 | train_loss 3.2188\n",
      "[window_stream] yielded=17000 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2130 batch_time=0.00s avg_batch_time=0.58s\n",
      "[estimate_loss] train 10 batches in 8.95s\n",
      "[window_stream] yielded=800 avg_time_per_sample=1.544s\n",
      "[batch_stream] batch=100 batch_time=0.00s avg_batch_time=12.35s\n",
      "[estimate_loss] val 10 batches in 8.98s\n",
      "[eval] step  1800 | train 3.3173 | val 6.1937\n",
      "[window_stream] yielded=17100 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2140 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1810 | lr 3.000e-06 | train_loss 2.6400\n",
      "[window_stream] yielded=17200 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2150 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1820 | lr 3.000e-06 | train_loss 7.2273\n",
      "[batch_stream] batch=2160 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1830 | lr 3.000e-06 | train_loss 6.1906\n",
      "[window_stream] yielded=17300 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2170 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1840 | lr 3.000e-06 | train_loss 6.5424\n",
      "[window_stream] yielded=17400 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2180 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1850 | lr 3.000e-06 | train_loss 6.2968\n",
      "[window_stream] yielded=17500 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2190 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1860 | lr 3.000e-06 | train_loss 7.1897\n",
      "[window_stream] yielded=17600 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2200 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1870 | lr 3.000e-06 | train_loss 7.0531\n",
      "[batch_stream] batch=2210 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1880 | lr 3.000e-06 | train_loss 6.4647\n",
      "[window_stream] yielded=17700 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2220 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  1890 | lr 3.000e-06 | train_loss 6.9053\n",
      "[window_stream] yielded=17800 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2230 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  1900 | lr 3.000e-06 | train_loss 6.3297\n",
      "[window_stream] yielded=17900 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2240 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1910 | lr 3.000e-06 | train_loss 6.4577\n",
      "[window_stream] yielded=18000 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2250 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1920 | lr 3.000e-06 | train_loss 6.7568\n",
      "[batch_stream] batch=2260 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1930 | lr 3.000e-06 | train_loss 6.0137\n",
      "[window_stream] yielded=18100 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2270 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1940 | lr 3.000e-06 | train_loss 7.3400\n",
      "[window_stream] yielded=18200 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2280 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  1950 | lr 3.000e-06 | train_loss 6.9561\n",
      "[window_stream] yielded=18300 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2290 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  1960 | lr 3.000e-06 | train_loss 7.0390\n",
      "[window_stream] yielded=18400 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2300 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  1970 | lr 3.000e-06 | train_loss 6.7864\n",
      "[batch_stream] batch=2310 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  1980 | lr 3.000e-06 | train_loss 7.0804\n",
      "[window_stream] yielded=18500 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2320 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  1990 | lr 3.000e-06 | train_loss 6.9004\n",
      "[window_stream] yielded=18600 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2330 batch_time=0.00s avg_batch_time=0.57s\n",
      "fp c9bb2c93a5\n",
      "step  2000 | lr 3.000e-06 | train_loss 6.5713\n",
      "[window_stream] tokens buffered=2400 total_seen=19100000 rate=14403 tok/s samples_yielded=18650\n",
      "[window_stream] yielded=18700 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2340 batch_time=0.00s avg_batch_time=0.57s\n",
      "[estimate_loss] train 10 batches in 8.97s\n",
      "[batch_stream] batch=110 batch_time=0.00s avg_batch_time=12.14s\n",
      "[estimate_loss] val 10 batches in 8.95s\n",
      "[eval] step  2000 | train 6.4321 | val 5.6228\n",
      "[window_stream] yielded=18800 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2350 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  2010 | lr 3.000e-06 | train_loss 5.8859\n",
      "[batch_stream] batch=2360 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  2020 | lr 3.000e-06 | train_loss 6.5330\n",
      "[window_stream] yielded=18900 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=2370 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  2030 | lr 3.000e-06 | train_loss 5.4507\n",
      "[window_stream] yielded=19000 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2380 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  2040 | lr 3.000e-06 | train_loss 6.0342\n",
      "[window_stream] yielded=19100 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2390 batch_time=0.01s avg_batch_time=0.57s\n",
      "step  2050 | lr 3.000e-06 | train_loss 5.8153\n",
      "[window_stream] yielded=19200 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2400 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  2060 | lr 3.000e-06 | train_loss 6.7820\n",
      "[batch_stream] batch=2410 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  2070 | lr 3.000e-06 | train_loss 4.4997\n",
      "[window_stream] yielded=19300 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2420 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  2080 | lr 3.000e-06 | train_loss 4.4665\n",
      "[window_stream] yielded=19400 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2430 batch_time=0.00s avg_batch_time=0.57s\n",
      "step  2090 | lr 3.000e-06 | train_loss 6.5120\n",
      "[window_stream] yielded=19500 avg_time_per_sample=0.071s\n",
      "[batch_stream] batch=2440 batch_time=0.06s avg_batch_time=0.57s\n",
      "step  2100 | lr 3.000e-06 | train_loss 6.8809\n",
      "[saved FP16 weights → weights/model_step_2100.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb541327af824bb8b1b71dc64623cbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57f6d15d0a2469aa0ddaa5a09096c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ead4cafb4c4734ac217576def53235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba20a1a6cabd4276ab286009c2f88c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded → hf://345rf4gt56t4r3e3/nnn/model_step_2100.pt]\n",
      "[window_stream] yielded=19600 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=2450 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  2110 | lr 3.000e-06 | train_loss 5.0834\n",
      "[batch_stream] batch=2460 batch_time=0.03s avg_batch_time=0.61s\n",
      "step  2120 | lr 3.000e-06 | train_loss 6.4235\n",
      "[window_stream] yielded=19700 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=2470 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  2130 | lr 3.000e-06 | train_loss 5.7862\n",
      "[window_stream] yielded=19800 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=2480 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2140 | lr 3.000e-06 | train_loss 5.6431\n",
      "[window_stream] yielded=19900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2490 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2150 | lr 3.000e-06 | train_loss 6.8243\n",
      "[window_stream] yielded=20000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2500 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2160 | lr 3.000e-06 | train_loss 5.9989\n",
      "[batch_stream] batch=2510 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2170 | lr 3.000e-06 | train_loss 5.7001\n",
      "[window_stream] yielded=20100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2520 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2180 | lr 3.000e-06 | train_loss 6.2042\n",
      "[window_stream] yielded=20200 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2530 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2190 | lr 3.000e-06 | train_loss 6.7609\n",
      "[window_stream] yielded=20300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2540 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2200 | lr 3.000e-06 | train_loss 2.4013\n",
      "[window_stream] yielded=20400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2550 batch_time=0.01s avg_batch_time=0.60s\n",
      "[estimate_loss] train 10 batches in 8.90s\n",
      "[window_stream] yielded=900 avg_time_per_sample=1.698s\n",
      "[batch_stream] batch=120 batch_time=0.01s avg_batch_time=12.78s\n",
      "[estimate_loss] val 10 batches in 8.93s\n",
      "[eval] step  2200 | train 5.2331 | val 6.4762\n",
      "[batch_stream] batch=2560 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2210 | lr 3.000e-06 | train_loss 5.9234\n",
      "[window_stream] yielded=20500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2570 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2220 | lr 3.000e-06 | train_loss 6.0273\n",
      "[window_stream] yielded=20600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2580 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2230 | lr 3.000e-06 | train_loss 3.4308\n",
      "[window_stream] yielded=20700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2590 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2240 | lr 3.000e-06 | train_loss 6.6570\n",
      "[window_stream] yielded=20800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2600 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2250 | lr 3.000e-06 | train_loss 5.3819\n",
      "[batch_stream] batch=2610 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2260 | lr 3.000e-06 | train_loss 6.8206\n",
      "[window_stream] yielded=20900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2620 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2270 | lr 3.000e-06 | train_loss 5.9670\n",
      "[window_stream] yielded=21000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2630 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2280 | lr 3.000e-06 | train_loss 6.6331\n",
      "[window_stream] yielded=21100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2640 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2290 | lr 3.000e-06 | train_loss 6.8044\n",
      "[window_stream] yielded=21200 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2650 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2300 | lr 3.000e-06 | train_loss 6.7992\n",
      "[batch_stream] batch=2660 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2310 | lr 3.000e-06 | train_loss 2.6444\n",
      "[window_stream] yielded=21300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=2670 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2320 | lr 3.000e-06 | train_loss 5.1331\n",
      "[window_stream] yielded=21400 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2680 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2330 | lr 3.000e-06 | train_loss 6.4549\n",
      "[window_stream] yielded=21500 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2690 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2340 | lr 3.000e-06 | train_loss 6.9704\n",
      "[window_stream] yielded=21600 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2700 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2350 | lr 3.000e-06 | train_loss 6.6676\n",
      "[batch_stream] batch=2710 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2360 | lr 3.000e-06 | train_loss 5.1458\n",
      "[window_stream] yielded=21700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2720 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2370 | lr 3.000e-06 | train_loss 7.0944\n",
      "[window_stream] yielded=21800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2730 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2380 | lr 3.000e-06 | train_loss 4.9817\n",
      "[window_stream] yielded=21900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2740 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2390 | lr 3.000e-06 | train_loss 6.4619\n",
      "[window_stream] yielded=22000 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2750 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2400 | lr 3.000e-06 | train_loss 4.4530\n",
      "[batch_stream] batch=2760 batch_time=0.00s avg_batch_time=0.59s\n",
      "[estimate_loss] train 10 batches in 8.97s\n",
      "[window_stream] yielded=1000 avg_time_per_sample=1.630s\n",
      "[batch_stream] batch=130 batch_time=0.01s avg_batch_time=12.57s\n",
      "[estimate_loss] val 10 batches in 8.98s\n",
      "[eval] step  2400 | train 6.6070 | val 5.8493\n",
      "[window_stream] yielded=22100 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2770 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2410 | lr 3.000e-06 | train_loss 6.5184\n",
      "[window_stream] yielded=22200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2780 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2420 | lr 3.000e-06 | train_loss 6.3631\n",
      "[window_stream] yielded=22300 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2790 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2430 | lr 3.000e-06 | train_loss 7.3215\n",
      "[window_stream] yielded=22400 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2800 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2440 | lr 3.000e-06 | train_loss 6.8453\n",
      "[batch_stream] batch=2810 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2450 | lr 3.000e-06 | train_loss 6.5270\n",
      "[window_stream] yielded=22500 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2820 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2460 | lr 3.000e-06 | train_loss 5.7821\n",
      "[window_stream] yielded=22600 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2830 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2470 | lr 3.000e-06 | train_loss 6.6572\n",
      "[window_stream] yielded=22700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2840 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2480 | lr 3.000e-06 | train_loss 6.6594\n",
      "[window_stream] yielded=22800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2850 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2490 | lr 3.000e-06 | train_loss 5.9782\n",
      "[batch_stream] batch=2860 batch_time=0.00s avg_batch_time=0.59s\n",
      "fp be7afd8eaa\n",
      "step  2500 | lr 3.000e-06 | train_loss 6.4423\n",
      "[window_stream] yielded=22900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=2870 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2510 | lr 3.000e-06 | train_loss 6.8954\n",
      "[window_stream] yielded=23000 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2880 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2520 | lr 3.000e-06 | train_loss 6.8604\n",
      "[window_stream] yielded=23100 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2890 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2530 | lr 3.000e-06 | train_loss 6.3946\n",
      "[window_stream] yielded=23200 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2900 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2540 | lr 3.000e-06 | train_loss 6.3378\n",
      "[batch_stream] batch=2910 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2550 | lr 3.000e-06 | train_loss 5.4947\n",
      "[window_stream] yielded=23300 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2920 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2560 | lr 3.000e-06 | train_loss 6.8944\n",
      "[window_stream] yielded=23400 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2930 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2570 | lr 3.000e-06 | train_loss 7.1239\n",
      "[window_stream] yielded=23500 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2940 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2580 | lr 3.000e-06 | train_loss 5.8292\n",
      "[window_stream] yielded=23600 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2950 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2590 | lr 3.000e-06 | train_loss 6.6596\n",
      "[batch_stream] batch=2960 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2600 | lr 3.000e-06 | train_loss 7.1065\n",
      "[window_stream] yielded=23700 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2970 batch_time=0.01s avg_batch_time=0.58s\n",
      "[estimate_loss] train 10 batches in 8.97s\n",
      "[window_stream] yielded=1100 avg_time_per_sample=1.577s\n",
      "[batch_stream] batch=140 batch_time=0.00s avg_batch_time=12.40s\n",
      "[estimate_loss] val 10 batches in 8.96s\n",
      "[eval] step  2600 | train 6.0064 | val 4.5811\n",
      "[window_stream] yielded=23800 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2980 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2610 | lr 3.000e-06 | train_loss 6.7273\n",
      "[window_stream] yielded=23900 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=2990 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  2620 | lr 3.000e-06 | train_loss 6.7308\n",
      "[window_stream] yielded=24000 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3000 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  2630 | lr 3.000e-06 | train_loss 5.3160\n",
      "[batch_stream] batch=3010 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2640 | lr 3.000e-06 | train_loss 6.0891\n",
      "[window_stream] yielded=24100 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3020 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2650 | lr 3.000e-06 | train_loss 7.1104\n",
      "[window_stream] yielded=24200 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3030 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  2660 | lr 3.000e-06 | train_loss 6.6670\n",
      "[window_stream] yielded=24300 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3040 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  2670 | lr 3.000e-06 | train_loss 7.1769\n",
      "[window_stream] yielded=24400 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3050 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2680 | lr 3.000e-06 | train_loss 5.1191\n",
      "[batch_stream] batch=3060 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  2690 | lr 3.000e-06 | train_loss 6.1334\n",
      "[window_stream] yielded=24500 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3070 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2700 | lr 3.000e-06 | train_loss 5.8817\n",
      "[window_stream] yielded=24600 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3080 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  2710 | lr 3.000e-06 | train_loss 5.2453\n",
      "[window_stream] yielded=24700 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3090 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  2720 | lr 3.000e-06 | train_loss 4.8747\n",
      "[window_stream] yielded=24800 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=3100 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  2730 | lr 3.000e-06 | train_loss 6.8588\n",
      "[batch_stream] batch=3110 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  2740 | lr 3.000e-06 | train_loss 2.3656\n",
      "[window_stream] yielded=24900 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=3120 batch_time=0.02s avg_batch_time=0.58s\n",
      "step  2750 | lr 3.000e-06 | train_loss 6.9415\n",
      "[window_stream] yielded=25000 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=3130 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2760 | lr 3.000e-06 | train_loss 5.5825\n",
      "[window_stream] yielded=25100 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=3140 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2770 | lr 3.000e-06 | train_loss 6.6595\n",
      "[window_stream] yielded=25200 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=3150 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2780 | lr 3.000e-06 | train_loss 6.7780\n",
      "[batch_stream] batch=3160 batch_time=0.00s avg_batch_time=0.58s\n",
      "step  2790 | lr 3.000e-06 | train_loss 5.1437\n",
      "[window_stream] yielded=25300 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=3170 batch_time=0.01s avg_batch_time=0.58s\n",
      "step  2800 | lr 3.000e-06 | train_loss 5.9119\n",
      "[window_stream] yielded=25400 avg_time_per_sample=0.072s\n",
      "[batch_stream] batch=3180 batch_time=0.01s avg_batch_time=0.58s\n",
      "[estimate_loss] train 10 batches in 8.98s\n",
      "[window_stream] yielded=1200 avg_time_per_sample=1.531s\n",
      "[batch_stream] batch=150 batch_time=0.00s avg_batch_time=12.25s\n",
      "[estimate_loss] val 10 batches in 8.98s\n",
      "[eval] step  2800 | train 5.3451 | val 5.8705\n",
      "[saved FP16 weights → weights/model_step_2800.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792214b45c98428fb59466101cbf3a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3eece334d6452286a923959ac06a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a7ce969edd43f49d5c086542cab831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0db5e6804a44cbb89c5f2225037698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded → hf://345rf4gt56t4r3e3/nnn/model_step_2800.pt]\n",
      "[window_stream] yielded=25500 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3190 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  2810 | lr 3.000e-06 | train_loss 6.2171\n",
      "[window_stream] yielded=25600 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3200 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  2820 | lr 3.000e-06 | train_loss 6.2830\n",
      "[batch_stream] batch=3210 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  2830 | lr 3.000e-06 | train_loss 6.5752\n",
      "[window_stream] yielded=25700 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3220 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  2840 | lr 3.000e-06 | train_loss 6.8074\n",
      "[window_stream] yielded=25800 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3230 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  2850 | lr 3.000e-06 | train_loss 6.1810\n",
      "[window_stream] yielded=25900 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3240 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  2860 | lr 3.000e-06 | train_loss 6.6266\n",
      "[window_stream] yielded=26000 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3250 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  2870 | lr 3.000e-06 | train_loss 5.0541\n",
      "[batch_stream] batch=3260 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  2880 | lr 3.000e-06 | train_loss 4.8739\n",
      "[window_stream] yielded=26100 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3270 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  2890 | lr 3.000e-06 | train_loss 6.5201\n",
      "[window_stream] yielded=26200 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3280 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  2900 | lr 3.000e-06 | train_loss 6.8045\n",
      "[window_stream] yielded=26300 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3290 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2910 | lr 3.000e-06 | train_loss 6.8108\n",
      "[window_stream] yielded=26400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3300 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2920 | lr 3.000e-06 | train_loss 6.8070\n",
      "[batch_stream] batch=3310 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2930 | lr 3.000e-06 | train_loss 3.6484\n",
      "[window_stream] yielded=26500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3320 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2940 | lr 3.000e-06 | train_loss 7.0508\n",
      "[window_stream] yielded=26600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3330 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  2950 | lr 3.000e-06 | train_loss 2.3491\n",
      "[window_stream] yielded=26700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3340 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2960 | lr 3.000e-06 | train_loss 5.9664\n",
      "[window_stream] yielded=26800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3350 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2970 | lr 3.000e-06 | train_loss 6.3973\n",
      "[batch_stream] batch=3360 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2980 | lr 3.000e-06 | train_loss 6.8832\n",
      "[window_stream] yielded=26900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3370 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  2990 | lr 3.000e-06 | train_loss 6.2553\n",
      "[window_stream] yielded=27000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3380 batch_time=0.00s avg_batch_time=0.60s\n",
      "fp f0e55015e9\n",
      "step  3000 | lr 3.000e-06 | train_loss 5.8817\n",
      "[window_stream] yielded=27100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3390 batch_time=0.01s avg_batch_time=0.60s\n",
      "[estimate_loss] train 10 batches in 8.95s\n",
      "[batch_stream] batch=160 batch_time=0.01s avg_batch_time=12.73s\n",
      "[estimate_loss] val 10 batches in 8.95s\n",
      "[eval] step  3000 | train 5.7449 | val 5.7378\n",
      "[window_stream] yielded=27200 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3400 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3010 | lr 3.000e-06 | train_loss 6.5561\n",
      "[batch_stream] batch=3410 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3020 | lr 3.000e-06 | train_loss 6.6971\n",
      "[window_stream] yielded=27300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3420 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3030 | lr 3.000e-06 | train_loss 5.3328\n",
      "[window_stream] yielded=27400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3430 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3040 | lr 3.000e-06 | train_loss 4.9603\n",
      "[window_stream] yielded=27500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3440 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3050 | lr 3.000e-06 | train_loss 5.4264\n",
      "[window_stream] yielded=27600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3450 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3060 | lr 3.000e-06 | train_loss 5.6051\n",
      "[batch_stream] batch=3460 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3070 | lr 3.000e-06 | train_loss 5.7173\n",
      "[window_stream] yielded=27700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3470 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3080 | lr 3.000e-06 | train_loss 5.4566\n",
      "[window_stream] yielded=27800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3480 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3090 | lr 3.000e-06 | train_loss 4.5348\n",
      "[window_stream] yielded=27900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3490 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3100 | lr 3.000e-06 | train_loss 6.2820\n",
      "[window_stream] yielded=28000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3500 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3110 | lr 3.000e-06 | train_loss 6.7256\n",
      "[batch_stream] batch=3510 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3120 | lr 3.000e-06 | train_loss 5.1260\n",
      "[window_stream] yielded=28100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=3520 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3130 | lr 3.000e-06 | train_loss 3.4170\n",
      "[window_stream] yielded=28200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3530 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3140 | lr 3.000e-06 | train_loss 5.9460\n",
      "[window_stream] yielded=28300 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3540 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3150 | lr 3.000e-06 | train_loss 6.3160\n",
      "[window_stream] yielded=28400 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3550 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3160 | lr 3.000e-06 | train_loss 5.3098\n",
      "[batch_stream] batch=3560 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3170 | lr 3.000e-06 | train_loss 6.2098\n",
      "[window_stream] yielded=28500 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3570 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3180 | lr 3.000e-06 | train_loss 4.4587\n",
      "[window_stream] yielded=28600 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3580 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3190 | lr 3.000e-06 | train_loss 6.1577\n",
      "[window_stream] yielded=28700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3590 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3200 | lr 3.000e-06 | train_loss 7.3050\n",
      "[window_stream] yielded=28800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3600 batch_time=0.01s avg_batch_time=0.59s\n",
      "[estimate_loss] train 10 batches in 9.03s\n",
      "[window_stream] yielded=1300 avg_time_per_sample=1.639s\n",
      "[batch_stream] batch=170 batch_time=0.00s avg_batch_time=12.57s\n",
      "[estimate_loss] val 10 batches in 9.04s\n",
      "[eval] step  3200 | train 5.9010 | val 5.1933\n",
      "[batch_stream] batch=3610 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3210 | lr 3.000e-06 | train_loss 6.0450\n",
      "[window_stream] yielded=28900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3620 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3220 | lr 3.000e-06 | train_loss 6.9107\n",
      "[window_stream] yielded=29000 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3630 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3230 | lr 3.000e-06 | train_loss 6.5321\n",
      "[window_stream] yielded=29100 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3640 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3240 | lr 3.000e-06 | train_loss 4.8086\n",
      "[window_stream] yielded=29200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3650 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3250 | lr 3.000e-06 | train_loss 7.0905\n",
      "[batch_stream] batch=3660 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3260 | lr 3.000e-06 | train_loss 6.6540\n",
      "[window_stream] yielded=29300 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3670 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3270 | lr 3.000e-06 | train_loss 2.5342\n",
      "[window_stream] yielded=29400 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3680 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3280 | lr 3.000e-06 | train_loss 4.5333\n",
      "[window_stream] yielded=29500 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3690 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3290 | lr 3.000e-06 | train_loss 6.1462\n",
      "[window_stream] yielded=29600 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3700 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3300 | lr 3.000e-06 | train_loss 6.4770\n",
      "[batch_stream] batch=3710 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3310 | lr 3.000e-06 | train_loss 6.8276\n",
      "[window_stream] yielded=29700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3720 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3320 | lr 3.000e-06 | train_loss 6.9882\n",
      "[window_stream] yielded=29800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3730 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3330 | lr 3.000e-06 | train_loss 5.9201\n",
      "[window_stream] yielded=29900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3740 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3340 | lr 3.000e-06 | train_loss 1.0869\n",
      "[window_stream] yielded=30000 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3750 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3350 | lr 3.000e-06 | train_loss 6.7378\n",
      "[batch_stream] batch=3760 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3360 | lr 3.000e-06 | train_loss 7.2334\n",
      "[window_stream] yielded=30100 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3770 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3370 | lr 3.000e-06 | train_loss 7.1415\n",
      "[window_stream] yielded=30200 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3780 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3380 | lr 3.000e-06 | train_loss 6.4080\n",
      "[window_stream] yielded=30300 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3790 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3390 | lr 3.000e-06 | train_loss 6.0691\n",
      "[window_stream] yielded=30400 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3800 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3400 | lr 3.000e-06 | train_loss 5.6826\n",
      "[token_stream] 20000 docs in 2232.1s (9.0 docs/s)\n",
      "[batch_stream] batch=3810 batch_time=0.02s avg_batch_time=0.59s\n",
      "[estimate_loss] train 10 batches in 9.02s\n",
      "[window_stream] yielded=1400 avg_time_per_sample=1.596s\n",
      "[batch_stream] batch=180 batch_time=0.00s avg_batch_time=12.44s\n",
      "[estimate_loss] val 10 batches in 9.00s\n",
      "[eval] step  3400 | train 6.1517 | val 6.3503\n",
      "[window_stream] yielded=30500 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3820 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3410 | lr 3.000e-06 | train_loss 8.7850\n",
      "[window_stream] yielded=30600 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3830 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3420 | lr 3.000e-06 | train_loss 6.4850\n",
      "[window_stream] yielded=30700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3840 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3430 | lr 3.000e-06 | train_loss 4.6988\n",
      "[window_stream] yielded=30800 avg_time_per_sample=0.073s\n",
      "[batch_stream] batch=3850 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3440 | lr 3.000e-06 | train_loss 7.4848\n",
      "[batch_stream] batch=3860 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3450 | lr 3.000e-06 | train_loss 5.0248\n",
      "[window_stream] yielded=30900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3870 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3460 | lr 3.000e-06 | train_loss 6.0750\n",
      "[window_stream] yielded=31000 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3880 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3470 | lr 3.000e-06 | train_loss 5.9824\n",
      "[window_stream] yielded=31100 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3890 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  3480 | lr 3.000e-06 | train_loss 3.0006\n",
      "[window_stream] yielded=31200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=3900 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  3490 | lr 3.000e-06 | train_loss 6.6664\n",
      "[batch_stream] batch=3910 batch_time=0.01s avg_batch_time=0.59s\n",
      "fp 978d0dd95b\n",
      "step  3500 | lr 3.000e-06 | train_loss 6.3271\n",
      "[saved FP16 weights → weights/model_step_3500.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4052e7e6083744758464788f7173f3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e88a120165483baefc5f5f0aa9ac8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c66c498c9f4824a942e98edfd7d286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876d46fe0f3a43e695c3861acf856cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded → hf://345rf4gt56t4r3e3/nnn/model_step_3500.pt]\n",
      "[window_stream] yielded=31300 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=3920 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3510 | lr 3.000e-06 | train_loss 6.8460\n",
      "[window_stream] yielded=31400 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=3930 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3520 | lr 3.000e-06 | train_loss 6.9041\n",
      "[window_stream] yielded=31500 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=3940 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3530 | lr 3.000e-06 | train_loss 5.3311\n",
      "[window_stream] yielded=31600 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3950 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3540 | lr 3.000e-06 | train_loss 4.8917\n",
      "[batch_stream] batch=3960 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3550 | lr 3.000e-06 | train_loss 6.7529\n",
      "[window_stream] yielded=31700 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3970 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3560 | lr 3.000e-06 | train_loss 7.0043\n",
      "[window_stream] yielded=31800 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3980 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3570 | lr 3.000e-06 | train_loss 6.8994\n",
      "[window_stream] yielded=31900 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=3990 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3580 | lr 3.000e-06 | train_loss 4.7787\n",
      "[window_stream] yielded=32000 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4000 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3590 | lr 3.000e-06 | train_loss 6.2479\n",
      "[batch_stream] batch=4010 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3600 | lr 3.000e-06 | train_loss 6.7518\n",
      "[window_stream] yielded=32100 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4020 batch_time=0.00s avg_batch_time=0.61s\n",
      "[estimate_loss] train 10 batches in 8.97s\n",
      "[window_stream] yielded=1500 avg_time_per_sample=1.632s\n",
      "[batch_stream] batch=190 batch_time=0.00s avg_batch_time=12.89s\n",
      "[estimate_loss] val 10 batches in 8.98s\n",
      "[eval] step  3600 | train 5.5977 | val 5.6398\n",
      "[window_stream] yielded=32200 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4030 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3610 | lr 3.000e-06 | train_loss 5.9153\n",
      "[window_stream] yielded=32300 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4040 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3620 | lr 3.000e-06 | train_loss 6.4305\n",
      "[window_stream] yielded=32400 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4050 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3630 | lr 3.000e-06 | train_loss 4.9379\n",
      "[batch_stream] batch=4060 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3640 | lr 3.000e-06 | train_loss 5.1602\n",
      "[window_stream] yielded=32500 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4070 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3650 | lr 3.000e-06 | train_loss 6.6420\n",
      "[window_stream] yielded=32600 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4080 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3660 | lr 3.000e-06 | train_loss 7.0217\n",
      "[window_stream] yielded=32700 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4090 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3670 | lr 3.000e-06 | train_loss 6.8117\n",
      "[window_stream] yielded=32800 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4100 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3680 | lr 3.000e-06 | train_loss 6.5030\n",
      "[batch_stream] batch=4110 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3690 | lr 3.000e-06 | train_loss 6.6406\n",
      "[window_stream] yielded=32900 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4120 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  3700 | lr 3.000e-06 | train_loss 6.7404\n",
      "[window_stream] yielded=33000 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4130 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3710 | lr 3.000e-06 | train_loss 6.7974\n",
      "[window_stream] yielded=33100 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4140 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3720 | lr 3.000e-06 | train_loss 0.4034\n",
      "[window_stream] yielded=33200 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4150 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  3730 | lr 3.000e-06 | train_loss 5.2914\n",
      "[batch_stream] batch=4160 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3740 | lr 3.000e-06 | train_loss 6.9227\n",
      "[window_stream] yielded=33300 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4170 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3750 | lr 3.000e-06 | train_loss 6.4967\n",
      "[window_stream] yielded=33400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4180 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3760 | lr 3.000e-06 | train_loss 8.7262\n",
      "[window_stream] yielded=33500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4190 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3770 | lr 3.000e-06 | train_loss 7.0874\n",
      "[window_stream] yielded=33600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4200 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3780 | lr 3.000e-06 | train_loss 5.2382\n",
      "[batch_stream] batch=4210 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3790 | lr 3.000e-06 | train_loss 5.2868\n",
      "[window_stream] yielded=33700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4220 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3800 | lr 3.000e-06 | train_loss 5.9666\n",
      "[window_stream] yielded=33800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4230 batch_time=0.01s avg_batch_time=0.60s\n",
      "[estimate_loss] train 10 batches in 8.99s\n",
      "[window_stream] yielded=1600 avg_time_per_sample=1.595s\n",
      "[batch_stream] batch=200 batch_time=0.01s avg_batch_time=12.76s\n",
      "[estimate_loss] val 10 batches in 8.99s\n",
      "[eval] step  3800 | train 6.3221 | val 6.6065\n",
      "[window_stream] yielded=33900 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4240 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3810 | lr 3.000e-06 | train_loss 4.7704\n",
      "[window_stream] yielded=34000 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4250 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3820 | lr 3.000e-06 | train_loss 6.0541\n",
      "[batch_stream] batch=4260 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3830 | lr 3.000e-06 | train_loss 5.1345\n",
      "[window_stream] yielded=34100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4270 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3840 | lr 3.000e-06 | train_loss 6.2550\n",
      "[window_stream] yielded=34200 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4280 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3850 | lr 3.000e-06 | train_loss 6.3244\n",
      "[window_stream] yielded=34300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4290 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3860 | lr 3.000e-06 | train_loss 6.0809\n",
      "[window_stream] yielded=34400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4300 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3870 | lr 3.000e-06 | train_loss 3.6271\n",
      "[batch_stream] batch=4310 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3880 | lr 3.000e-06 | train_loss 5.5407\n",
      "[window_stream] yielded=34500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4320 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3890 | lr 3.000e-06 | train_loss 6.4276\n",
      "[window_stream] yielded=34600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4330 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3900 | lr 3.000e-06 | train_loss 6.3182\n",
      "[window_stream] yielded=34700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4340 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3910 | lr 3.000e-06 | train_loss 6.8518\n",
      "[window_stream] yielded=34800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4350 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3920 | lr 3.000e-06 | train_loss 6.1468\n",
      "[batch_stream] batch=4360 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3930 | lr 3.000e-06 | train_loss 6.2704\n",
      "[window_stream] yielded=34900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4370 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3940 | lr 3.000e-06 | train_loss 5.1781\n",
      "[window_stream] yielded=35000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4380 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3950 | lr 3.000e-06 | train_loss 5.2459\n",
      "[window_stream] yielded=35100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4390 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3960 | lr 3.000e-06 | train_loss 4.8069\n",
      "[window_stream] yielded=35200 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4400 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3970 | lr 3.000e-06 | train_loss 6.0529\n",
      "[batch_stream] batch=4410 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  3980 | lr 3.000e-06 | train_loss 1.9457\n",
      "[window_stream] yielded=35300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4420 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  3990 | lr 3.000e-06 | train_loss 3.0109\n",
      "[window_stream] yielded=35400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4430 batch_time=0.01s avg_batch_time=0.60s\n",
      "fp 40e7a1bc8b\n",
      "step  4000 | lr 3.000e-06 | train_loss 6.7461\n",
      "[window_stream] yielded=35500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4440 batch_time=0.00s avg_batch_time=0.60s\n",
      "[estimate_loss] train 10 batches in 8.97s\n",
      "[batch_stream] batch=210 batch_time=0.01s avg_batch_time=12.63s\n",
      "[estimate_loss] val 10 batches in 8.99s\n",
      "[eval] step  4000 | train 6.1663 | val 6.0873\n",
      "[window_stream] yielded=35600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4450 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4010 | lr 3.000e-06 | train_loss 5.3537\n",
      "[batch_stream] batch=4460 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4020 | lr 3.000e-06 | train_loss 4.5321\n",
      "[window_stream] yielded=35700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4470 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4030 | lr 3.000e-06 | train_loss 7.0385\n",
      "[window_stream] yielded=35800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4480 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4040 | lr 3.000e-06 | train_loss 0.2989\n",
      "[window_stream] yielded=35900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4490 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4050 | lr 3.000e-06 | train_loss 0.3248\n",
      "[window_stream] yielded=36000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4500 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4060 | lr 3.000e-06 | train_loss 6.3112\n",
      "[batch_stream] batch=4510 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4070 | lr 3.000e-06 | train_loss 6.4300\n",
      "[window_stream] yielded=36100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4520 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4080 | lr 3.000e-06 | train_loss 4.9079\n",
      "[window_stream] yielded=36200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4530 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4090 | lr 3.000e-06 | train_loss 5.8351\n",
      "[window_stream] yielded=36300 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4540 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  4100 | lr 3.000e-06 | train_loss 3.6764\n",
      "[window_stream] yielded=36400 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4550 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  4110 | lr 3.000e-06 | train_loss 2.3793\n",
      "[batch_stream] batch=4560 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4120 | lr 3.000e-06 | train_loss 6.8975\n",
      "[window_stream] yielded=36500 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4570 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4130 | lr 3.000e-06 | train_loss 6.3962\n",
      "[window_stream] yielded=36600 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4580 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4140 | lr 3.000e-06 | train_loss 6.5707\n",
      "[window_stream] yielded=36700 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4590 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4150 | lr 3.000e-06 | train_loss 6.2367\n",
      "[window_stream] yielded=36800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4600 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4160 | lr 3.000e-06 | train_loss 6.6681\n",
      "[batch_stream] batch=4610 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4170 | lr 3.000e-06 | train_loss 6.2689\n",
      "[window_stream] yielded=36900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4620 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4180 | lr 3.000e-06 | train_loss 6.4420\n",
      "[window_stream] yielded=37000 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4630 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4190 | lr 3.000e-06 | train_loss 5.9704\n",
      "[window_stream] yielded=37100 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4640 batch_time=0.00s avg_batch_time=0.59s\n",
      "step  4200 | lr 3.000e-06 | train_loss 4.6085\n",
      "[window_stream] yielded=37200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=4650 batch_time=0.00s avg_batch_time=0.59s\n",
      "[estimate_loss] train 10 batches in 8.94s\n",
      "[window_stream] yielded=1700 avg_time_per_sample=1.615s\n",
      "[batch_stream] batch=220 batch_time=0.00s avg_batch_time=12.51s\n",
      "[estimate_loss] val 10 batches in 8.93s\n",
      "[eval] step  4200 | train 4.8557 | val 5.2008\n",
      "[saved FP16 weights → weights/model_step_4200.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02edc8bcecc046d7a771e33df73deb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d18b168e9f4fe99b2c4975ed318d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5aee94e5a9432bbf2bbdfee0ca062c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1638da5f43484c549c9f3951e943f3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded → hf://345rf4gt56t4r3e3/nnn/model_step_4200.pt]\n",
      "[batch_stream] batch=4660 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4210 | lr 3.000e-06 | train_loss 5.8252\n",
      "[window_stream] yielded=37300 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=4670 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4220 | lr 3.000e-06 | train_loss 5.5544\n",
      "[window_stream] yielded=37400 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=4680 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4230 | lr 3.000e-06 | train_loss 6.7073\n",
      "[window_stream] yielded=37500 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=4690 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4240 | lr 3.000e-06 | train_loss 5.0921\n",
      "[window_stream] yielded=37600 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4700 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4250 | lr 3.000e-06 | train_loss 6.5903\n",
      "[batch_stream] batch=4710 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4260 | lr 3.000e-06 | train_loss 6.7459\n",
      "[window_stream] yielded=37700 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4720 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4270 | lr 3.000e-06 | train_loss 4.0486\n",
      "[window_stream] yielded=37800 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4730 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4280 | lr 3.000e-06 | train_loss 6.3369\n",
      "[window_stream] yielded=37900 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4740 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4290 | lr 3.000e-06 | train_loss 6.1685\n",
      "[window_stream] yielded=38000 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4750 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4300 | lr 3.000e-06 | train_loss 4.8558\n",
      "[batch_stream] batch=4760 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4310 | lr 3.000e-06 | train_loss 6.7141\n",
      "[window_stream] yielded=38100 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4770 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4320 | lr 3.000e-06 | train_loss 5.8787\n",
      "[window_stream] yielded=38200 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4780 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4330 | lr 3.000e-06 | train_loss 5.0617\n",
      "[window_stream] yielded=38300 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4790 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4340 | lr 3.000e-06 | train_loss 6.4848\n",
      "[window_stream] yielded=38400 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4800 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4350 | lr 3.000e-06 | train_loss 7.4047\n",
      "[batch_stream] batch=4810 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4360 | lr 3.000e-06 | train_loss 5.0053\n",
      "[window_stream] yielded=38500 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4820 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4370 | lr 3.000e-06 | train_loss 6.3416\n",
      "[window_stream] yielded=38600 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4830 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4380 | lr 3.000e-06 | train_loss 5.5294\n",
      "[window_stream] yielded=38700 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4840 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4390 | lr 3.000e-06 | train_loss 5.1414\n",
      "[window_stream] yielded=38800 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4850 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4400 | lr 3.000e-06 | train_loss 7.1091\n",
      "[batch_stream] batch=4860 batch_time=0.00s avg_batch_time=0.61s\n",
      "[estimate_loss] train 10 batches in 8.90s\n",
      "[window_stream] yielded=1800 avg_time_per_sample=1.635s\n",
      "[batch_stream] batch=230 batch_time=0.01s avg_batch_time=12.82s\n",
      "[estimate_loss] val 10 batches in 9.79s\n",
      "[eval] step  4400 | train 6.5821 | val 6.0539\n",
      "[window_stream] yielded=38900 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4870 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4410 | lr 3.000e-06 | train_loss 5.8574\n",
      "[window_stream] yielded=39000 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4880 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4420 | lr 3.000e-06 | train_loss 6.5782\n",
      "[window_stream] yielded=39100 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4890 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4430 | lr 3.000e-06 | train_loss 3.9698\n",
      "[window_stream] yielded=39200 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4900 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4440 | lr 3.000e-06 | train_loss 6.2395\n",
      "[batch_stream] batch=4910 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4450 | lr 3.000e-06 | train_loss 5.3527\n",
      "[window_stream] yielded=39300 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4920 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4460 | lr 3.000e-06 | train_loss 6.5120\n",
      "[window_stream] yielded=39400 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4930 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4470 | lr 3.000e-06 | train_loss 5.7239\n",
      "[window_stream] yielded=39500 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4940 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4480 | lr 3.000e-06 | train_loss 6.5102\n",
      "[window_stream] yielded=39600 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4950 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4490 | lr 3.000e-06 | train_loss 5.4824\n",
      "[batch_stream] batch=4960 batch_time=0.00s avg_batch_time=0.60s\n",
      "fp 8b853d3c07\n",
      "step  4500 | lr 3.000e-06 | train_loss 5.0814\n",
      "[window_stream] yielded=39700 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=4970 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4510 | lr 3.000e-06 | train_loss 6.2451\n",
      "[window_stream] yielded=39800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4980 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4520 | lr 3.000e-06 | train_loss 6.4846\n",
      "[window_stream] yielded=39900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=4990 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4530 | lr 3.000e-06 | train_loss 6.5780\n",
      "[window_stream] yielded=40000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5000 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4540 | lr 3.000e-06 | train_loss 6.2993\n",
      "[batch_stream] batch=5010 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4550 | lr 3.000e-06 | train_loss 5.8384\n",
      "[window_stream] yielded=40100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5020 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4560 | lr 3.000e-06 | train_loss 6.1574\n",
      "[window_stream] yielded=40200 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5030 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4570 | lr 3.000e-06 | train_loss 6.1614\n",
      "[window_stream] yielded=40300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5040 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4580 | lr 3.000e-06 | train_loss 3.8873\n",
      "[window_stream] yielded=40400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5050 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4590 | lr 3.000e-06 | train_loss 6.4547\n",
      "[batch_stream] batch=5060 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4600 | lr 3.000e-06 | train_loss 6.2064\n",
      "[window_stream] yielded=40500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5070 batch_time=0.01s avg_batch_time=0.60s\n",
      "[estimate_loss] train 10 batches in 8.93s\n",
      "[window_stream] yielded=1900 avg_time_per_sample=1.604s\n",
      "[batch_stream] batch=240 batch_time=0.00s avg_batch_time=12.71s\n",
      "[estimate_loss] val 10 batches in 8.95s\n",
      "[eval] step  4600 | train 6.2096 | val 5.7378\n",
      "[window_stream] yielded=40600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5080 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4610 | lr 3.000e-06 | train_loss 6.3305\n",
      "[window_stream] yielded=40700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5090 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4620 | lr 3.000e-06 | train_loss 3.7774\n",
      "[window_stream] yielded=40800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5100 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4630 | lr 3.000e-06 | train_loss 6.7134\n",
      "[batch_stream] batch=5110 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4640 | lr 3.000e-06 | train_loss 6.3331\n",
      "[window_stream] yielded=40900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5120 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4650 | lr 3.000e-06 | train_loss 2.9771\n",
      "[window_stream] yielded=41000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5130 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4660 | lr 3.000e-06 | train_loss 6.4191\n",
      "[window_stream] yielded=41100 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5140 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4670 | lr 3.000e-06 | train_loss 5.1083\n",
      "[window_stream] yielded=41200 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5150 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4680 | lr 3.000e-06 | train_loss 5.4922\n",
      "[batch_stream] batch=5160 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4690 | lr 3.000e-06 | train_loss 6.7166\n",
      "[window_stream] yielded=41300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5170 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4700 | lr 3.000e-06 | train_loss 5.9653\n",
      "[window_stream] yielded=41400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5180 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4710 | lr 3.000e-06 | train_loss 6.5132\n",
      "[window_stream] yielded=41500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5190 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4720 | lr 3.000e-06 | train_loss 5.1280\n",
      "[window_stream] yielded=41600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5200 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4730 | lr 3.000e-06 | train_loss 3.8883\n",
      "[batch_stream] batch=5210 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4740 | lr 3.000e-06 | train_loss 6.4263\n",
      "[window_stream] yielded=41700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5220 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4750 | lr 3.000e-06 | train_loss 3.4086\n",
      "[window_stream] yielded=41800 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5230 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4760 | lr 3.000e-06 | train_loss 6.6623\n",
      "[window_stream] yielded=41900 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5240 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4770 | lr 3.000e-06 | train_loss 4.8392\n",
      "[window_stream] yielded=42000 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5250 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4780 | lr 3.000e-06 | train_loss 6.0243\n",
      "[batch_stream] batch=5260 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4790 | lr 3.000e-06 | train_loss 6.4301\n",
      "[window_stream] yielded=42100 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=5270 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4800 | lr 3.000e-06 | train_loss 6.3905\n",
      "[window_stream] yielded=42200 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=5280 batch_time=0.00s avg_batch_time=0.60s\n",
      "[estimate_loss] train 10 batches in 8.92s\n",
      "[window_stream] yielded=2000 avg_time_per_sample=1.575s\n",
      "[batch_stream] batch=250 batch_time=0.01s avg_batch_time=12.60s\n",
      "[estimate_loss] val 10 batches in 8.90s\n",
      "[eval] step  4800 | train 6.0163 | val 6.2964\n",
      "[window_stream] yielded=42300 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5290 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4810 | lr 3.000e-06 | train_loss 4.7976\n",
      "[window_stream] yielded=42400 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5300 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4820 | lr 3.000e-06 | train_loss 6.3715\n",
      "[batch_stream] batch=5310 batch_time=0.00s avg_batch_time=0.60s\n",
      "step  4830 | lr 3.000e-06 | train_loss 6.5078\n",
      "[window_stream] yielded=42500 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5320 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4840 | lr 3.000e-06 | train_loss 6.1097\n",
      "[window_stream] yielded=42600 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5330 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4850 | lr 3.000e-06 | train_loss 6.1227\n",
      "[window_stream] yielded=42700 avg_time_per_sample=0.075s\n",
      "[batch_stream] batch=5340 batch_time=0.02s avg_batch_time=0.60s\n",
      "step  4860 | lr 3.000e-06 | train_loss 6.4477\n",
      "[window_stream] yielded=42800 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=5350 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4870 | lr 3.000e-06 | train_loss 6.0617\n",
      "[batch_stream] batch=5360 batch_time=0.01s avg_batch_time=0.60s\n",
      "step  4880 | lr 3.000e-06 | train_loss 6.3049\n",
      "[window_stream] yielded=42900 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=5370 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4890 | lr 3.000e-06 | train_loss 6.1781\n",
      "[window_stream] yielded=43000 avg_time_per_sample=0.074s\n",
      "[batch_stream] batch=5380 batch_time=0.01s avg_batch_time=0.59s\n",
      "step  4900 | lr 3.000e-06 | train_loss 5.8855\n",
      "[saved FP16 weights → weights/model_step_4900.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e0e7c2d14748d9b7caf54721c09f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca3b6c86ab24e989d47ddcf55ac88eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e712e1aef34713a0b0a58a35d84961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf28d8338b084d8b8cbe32df763c9dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded → hf://345rf4gt56t4r3e3/nnn/model_step_4900.pt]\n",
      "[window_stream] yielded=43100 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=5390 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4910 | lr 3.000e-06 | train_loss 6.4536\n",
      "[window_stream] yielded=43200 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=5400 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4920 | lr 3.000e-06 | train_loss 5.8772\n",
      "[batch_stream] batch=5410 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4930 | lr 3.000e-06 | train_loss 5.8425\n",
      "[window_stream] yielded=43300 avg_time_per_sample=0.077s\n",
      "[batch_stream] batch=5420 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4940 | lr 3.000e-06 | train_loss 5.9022\n",
      "[window_stream] yielded=43400 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=5430 batch_time=0.00s avg_batch_time=0.61s\n",
      "step  4950 | lr 3.000e-06 | train_loss 1.5981\n",
      "[window_stream] yielded=43500 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=5440 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4960 | lr 3.000e-06 | train_loss 6.4623\n",
      "[window_stream] yielded=43600 avg_time_per_sample=0.076s\n",
      "[batch_stream] batch=5450 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4970 | lr 3.000e-06 | train_loss 5.2719\n",
      "[batch_stream] batch=5460 batch_time=0.01s avg_batch_time=0.61s\n",
      "step  4980 | lr 3.000e-06 | train_loss 6.3833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# ---- backward (AMP if cuda) ----\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# unscale -> clip -> step\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "CKPT_DIR   = \"weights\"\n",
    "SAVE_EVERY = 700\n",
    "VAL_EVERY  = 200          # evaluate train/val loss every N steps\n",
    "EVAL_BATCHES = 10         # how many batches to average for eval\n",
    "MAX_STEPS  = 9000\n",
    "GRAD_CLIP  = 1.0          # helps stability\n",
    "# ---------------------------\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if device.type == \"cuda\" else None\n",
    "\n",
    "history = []\n",
    "model.train()\n",
    "\n",
    "for step in tqdm(range(MAX_STEPS)):\n",
    "    # ---- LR schedule ----\n",
    "    lr_now = get_lr(step, MAX_STEPS)\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = lr_now\n",
    "\n",
    "    # ---- get batch ----\n",
    "    xb, yb = next(train_batch_iter)  # ✅ train stream only\n",
    "    xb = xb.to(device, non_blocking=True)\n",
    "    yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "    # optional cheap loop health check\n",
    "    if step % 500 == 0:\n",
    "        print(\"fp\", batch_fingerprint(xb))\n",
    "\n",
    "    # ---- forward ----\n",
    "    if device.type == \"cuda\":\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            _, loss = model(xb, yb)\n",
    "    else:\n",
    "        _, loss = model(xb, yb)\n",
    "\n",
    "    # ---- safety: NaN/Inf loss ----\n",
    "    if not torch.isfinite(loss):\n",
    "        print(f\"[FATAL] non-finite loss at step {step}: {loss.item()}\")\n",
    "        # dump a debug batch\n",
    "        torch.save({\"xb\": xb.detach().cpu(), \"yb\": yb.detach().cpu()}, os.path.join(CKPT_DIR, f\"bad_batch_step_{step}.pt\"))\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # ---- backward (AMP if cuda) ----\n",
    "    if device.type == \"cuda\":\n",
    "        scaler.scale(loss).backward()\n",
    "        # unscale -> clip -> step\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---- logging ----\n",
    "    loss_val = loss.item()\n",
    "    history.append(loss_val)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step:5d} | lr {lr_now:.3e} | train_loss {loss_val:.4f}\")\n",
    "\n",
    "    # ---- periodic eval (REAL val) ----\n",
    "    if step > 0 and step % VAL_EVERY == 0:\n",
    "        stats = estimate_loss_from_iters(\n",
    "            model,\n",
    "            train_batch_iter,\n",
    "            val_batch_iter,\n",
    "            eval_batches=EVAL_BATCHES,\n",
    "            device=device.type,\n",
    "        )\n",
    "        print(f\"[eval] step {step:5d} | train {stats['train']:.4f} | val {stats['val']:.4f}\")\n",
    "\n",
    "    # ---- SAVE *WEIGHTS ONLY* (SAFE) ----\n",
    "    if step > 0 and step % SAVE_EVERY == 0:\n",
    "        fname = f\"model_step_{step}.pt\"\n",
    "        fpath = os.path.join(CKPT_DIR, fname)\n",
    "\n",
    "        # 🔒 SAFE FP16 EXPORT (does NOT touch training model)\n",
    "        state_fp16 = {k: v.detach().half().cpu() for k, v in model.state_dict().items()}\n",
    "        torch.save(state_fp16, fpath)\n",
    "        print(f\"[saved FP16 weights → {fpath}]\")\n",
    "\n",
    "        # upload versioned snapshot\n",
    "        upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=fname,\n",
    "            repo_id=HF_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"weights @ step {step}\",\n",
    "        )\n",
    "\n",
    "        # update rolling pointer\n",
    "        upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=\"latest.pt\",\n",
    "            repo_id=HF_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"update latest @ step {step}\",\n",
    "        )\n",
    "\n",
    "        print(f\"[uploaded → hf://{HF_REPO}/{fname}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8GG-GT7_Pwz"
   },
   "outputs": [],
   "source": [
    "print(xb[0][:10])\n",
    "print(yb[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "[batch_stream] batch=310 batch_time=0.00s avg_batch_time=11.04s\n",
      "[window_stream] yielded=2500 avg_time_per_sample=1.369s\n",
      "[batch_stream] batch=320 batch_time=0.00s avg_batch_time=10.69s\n",
      "[window_stream] yielded=2600 avg_time_per_sample=1.316s\n",
      "[batch_stream] batch=330 batch_time=0.00s avg_batch_time=10.37s\n",
      "[window_stream] yielded=2700 avg_time_per_sample=1.267s\n",
      "[batch_stream] batch=340 batch_time=0.01s avg_batch_time=10.06s\n",
      "[window_stream] yielded=2800 avg_time_per_sample=1.222s\n",
      "[batch_stream] batch=350 batch_time=0.01s avg_batch_time=9.78s\n",
      "cached fixed_val batches: 50\n",
      "\n",
      "=== weights/model_step_700.pt ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2406/688559621.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: {'loss': 6.854935159683228, 'ppl': 948.5506230881616, 'acc': 0.16411376953125, 'tokens': 409600, 'batches': 50} (time 5.80s)\n",
      "\n",
      "=== weights/model_step_2800.pt ===\n",
      "val: {'loss': 6.150047655105591, 'ppl': 468.7397240912154, 'acc': 0.19770751953125, 'tokens': 409600, 'batches': 50} (time 5.80s)\n",
      "\n",
      "=== weights/model_step_3500.pt ===\n",
      "val: {'loss': 6.025877189636231, 'ppl': 414.0046433717589, 'acc': 0.2038427734375, 'tokens': 409600, 'batches': 50} (time 5.80s)\n",
      "\n",
      "=== weights/model_step_4200.pt ===\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def seed_all(seed=1337):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all(1337)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# How many fixed batches to evaluate on.\n",
    "# 50 is already much more stable than your current 10.\n",
    "FIXED_VAL_BATCHES = 50\n",
    "\n",
    "# Assumes you already have val_batch_iter defined (as in your notebook).\n",
    "# This consumes from the stream ONCE and stores batches for repeatable evals.\n",
    "fixed_val = []\n",
    "for i in range(FIXED_VAL_BATCHES):\n",
    "    xb, yb = next(val_batch_iter)  # <- your existing iterator\n",
    "    fixed_val.append((xb.cpu(), yb.cpu()))\n",
    "print(f\"cached fixed_val batches: {len(fixed_val)}\")\n",
    "import math\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_batches(model, batches, device=\"cuda\", amp=True):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for xb_cpu, yb_cpu in batches:\n",
    "        xb = xb_cpu.to(device, non_blocking=True)\n",
    "        yb = yb_cpu.to(device, non_blocking=True)\n",
    "\n",
    "        if device == \"cuda\" and amp:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                logits, loss = model(xb, yb)\n",
    "        else:\n",
    "            logits, loss = model(xb, yb)\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "        # --- Make logits comparable to yb for accuracy ---\n",
    "        # yb is (B, T)\n",
    "        B, T = yb.shape\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            # logits: (B, T, V)\n",
    "            preds = torch.argmax(logits, dim=-1)          # (B, T)\n",
    "            correct += int((preds == yb).sum().item())\n",
    "            total += int(yb.numel())\n",
    "\n",
    "        elif logits.dim() == 2:\n",
    "            # logits: either (B*T, V) OR (B, V) for last token\n",
    "            # If (B*T, V), compare to flattened yb\n",
    "            if logits.shape[0] == B * T:\n",
    "                preds = torch.argmax(logits, dim=-1)      # (B*T,)\n",
    "                y_flat = yb.reshape(-1)                   # (B*T,)\n",
    "                correct += int((preds == y_flat).sum().item())\n",
    "                total += int(y_flat.numel())\n",
    "            elif logits.shape[0] == B:\n",
    "                # last-token-only logits (rare in training forward)\n",
    "                # compare only last token of yb\n",
    "                preds = torch.argmax(logits, dim=-1)      # (B,)\n",
    "                correct += int((preds == yb[:, -1]).sum().item())\n",
    "                total += int(B)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected logits shape {tuple(logits.shape)} for yb {tuple(yb.shape)}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected logits dim {logits.dim()} with shape {tuple(logits.shape)}\")\n",
    "\n",
    "    mean_loss = total_loss / len(batches)\n",
    "    ppl = math.exp(mean_loss) if mean_loss < 50 else float(\"inf\")\n",
    "    acc = correct / total if total > 0 else float(\"nan\")\n",
    "\n",
    "    return {\"loss\": mean_loss, \"ppl\": ppl, \"acc\": acc, \"tokens\": total, \"batches\": len(batches)}\n",
    "\n",
    "def load_fp16_state_into_model(model, ckpt_path, device=\"cuda\"):\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(\"WARNING load_state_dict mismatch\")\n",
    "        print(\"missing keys:\", missing[:10], \"...\" if len(missing) > 10 else \"\")\n",
    "        print(\"unexpected keys:\", unexpected[:10], \"...\" if len(unexpected) > 10 else \"\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Put paths you want to compare here:\n",
    "CKPTS = [\n",
    "      \"weights/model_step_700.pt\",\n",
    "    \"weights/model_step_2800.pt\",\n",
    "    \"weights/model_step_3500.pt\",\n",
    "    \"weights/model_step_4200.pt\",\n",
    "    # add more if you have them:\n",
    "    \"weights/model_step_4900.pt\",\n",
    "]\n",
    "results = {}\n",
    "\n",
    "for p in CKPTS:\n",
    "    assert os.path.exists(p), f\"missing: {p}\"\n",
    "    print(\"\\n===\", p, \"===\")\n",
    "\n",
    "    # IMPORTANT: build a fresh model instance each time to avoid any weirdness\n",
    "    # Replace TransformerLM(...) with however you construct your model in the notebook\n",
    "    model = TransformerLM().to(device)\n",
    "\n",
    "    load_fp16_state_into_model(model, p, device=device)\n",
    "\n",
    "    t0 = time.time()\n",
    "    val_metrics = eval_batches(model, fixed_val, device=device, amp=True)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    out = {\"val\": val_metrics, \"sec\": dt}\n",
    "\n",
    "    # Optional train-eval slice\n",
    "    if \"fixed_train_eval\" in globals():\n",
    "        t1 = time.time()\n",
    "        tr_metrics = eval_batches(model, fixed_train_eval, device=device, amp=True)\n",
    "        out[\"train_eval\"] = tr_metrics\n",
    "        out[\"sec_train_eval\"] = time.time() - t1\n",
    "\n",
    "    results[p] = out\n",
    "\n",
    "    print(\"val:\", val_metrics, f\"(time {dt:.2f}s)\")\n",
    "    if \"train_eval\" in out:\n",
    "        print(\"train_eval:\", out[\"train_eval\"], f\"(time {out['sec_train_eval']:.2f}s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2mciRDW_711"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def complete(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # encode prompt\n",
    "    idx = torch.tensor(\n",
    "        [tokenizer.encode(prompt)],\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop context if needed\n",
    "        idx_cond = idx[:, -context_len :]\n",
    "\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # decode only the completion\n",
    "    completion = tokenizer.decode(idx[0].tolist())\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fnuym0M-dgMX"
   },
   "outputs": [],
   "source": [
    "from tiktoken import get_encoding\n",
    "\n",
    "\n",
    "text = complete(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=\"Blue is a color\",\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlhiyAIreTvx"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh96fd9xec7v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
