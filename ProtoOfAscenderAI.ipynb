{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLAWn2V4ZpIn"
   },
   "source": [
    "# 3.4B transformer based on pile uncopyrighted streamed via huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo5lerbom6dj",
    "outputId": "6f5855ee-fe33-465f-beb2-41184cd9bd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
      "Requirement already satisfied: fsspec[compression] in /usr/local/lib/python3.11/dist-packages (2024.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "\u001b[33mWARNING: fsspec 2024.2.0 does not provide the extra 'compression'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken tqdm datasets tiktoken zstandard \"fsspec[compression]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvKFeUoRngtR"
   },
   "source": [
    "# RESTART THE KERNEL IF RUNNING ON RUNPOD AFTER THIS PIP INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9enT73mnYoiM"
   },
   "outputs": [],
   "source": [
    "# -------- MODEL PARAMS --------\n",
    "n_layers    = 64\n",
    "n_embd      = 2048\n",
    "n_heads     = 16\n",
    "context_len = 1024\n",
    "batch_size  = 8    # with grad accumulation\n",
    "dropout     = 0\n",
    "lr          = 3e-6\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "vocab_size  = tokenizer.n_vocab\n",
    "\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxN4QEpOCCHN"
   },
   "source": [
    "# *cool lr sinusoid trick*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AQ3o30qoZIW6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr(step, max_steps, base_lr=lr, warmup_steps=2000):\n",
    "    \"\"\"\n",
    "    Warmup + cosine decay learning rate schedule.\n",
    "\n",
    "    Args:\n",
    "        step (int): current training step\n",
    "        max_steps (int): total training steps\n",
    "        base_lr (float): peak learning rate\n",
    "        warmup_steps (int): number of warmup steps\n",
    "\n",
    "    Returns:\n",
    "        float: learning rate for this step\n",
    "    \"\"\"\n",
    "    if step < warmup_steps:\n",
    "        return base_lr * step / warmup_steps\n",
    "\n",
    "    progress = (step - warmup_steps) / max(1, max_steps - warmup_steps)\n",
    "    return base_lr * 0.5 * (1.0 + math.cos(math.pi * progress))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YNClLsseWNq-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "478tzhmPyb8g"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token(x)                    # (B, T, C)\n",
    "        pos = self.pos(torch.arange(T))        # (T, C)\n",
    "        return tok + pos\n",
    "emb = TokenEmbedding()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YL_tG3Xy1Bk"
   },
   "source": [
    "# Above is the definition of our embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ugc-43hy6Ak"
   },
   "source": [
    "# Now define attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tW4hfpmPyh-0"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)                       # (B, T, 3C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # this is the critical line\n",
    "        y = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            is_causal=True,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "        )\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO3ojwF2yvFb"
   },
   "source": [
    "# Constants again and positional embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-TUsNPmp2iWA"
   },
   "outputs": [],
   "source": [
    "class TokenPosEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token(x)                                # (B,T,C)\n",
    "        pos = self.pos(torch.arange(T, device=x.device))  # (T,C)\n",
    "        return tok + pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Jj6mrYi7M9Z"
   },
   "source": [
    "# After attention we have basic FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AIBpH7V57QfZ"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rdJuWGal6g6Z"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n",
    "        self.ff   = FeedForward()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O59GSjd07CcU"
   },
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # token + positional embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block() for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # final normalization + LM head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        # embeddings\n",
    "        tok = self.token_emb(x)                              # (B,T,C)\n",
    "        pos = self.pos_emb(torch.arange(T, device=x.device))# (T,C)\n",
    "        x = tok + pos                                        # (B,T,C)\n",
    "\n",
    "        # APPLY ALL BLOCKS HERE\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # final projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                                # (B,T,vocab)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits = logits.view(B*T, vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w9471Vfr8CrW"
   },
   "outputs": [],
   "source": [
    "model = TransformerLM().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oWWhQhrG8pj8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3430.409297 M params\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M params\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xakqTi7NCJsq"
   },
   "source": [
    "# optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "36Uwu8ox8yXJ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2LPa6zEb88eE"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        losses = []\n",
    "        for _ in range(20):\n",
    "            xb, yb = get_batch(split)\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train() # go back because we will be in a train loop\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zPfmgofSbChr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f2c9724bd24350b46b3f74c0eafeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def token_stream(hf_dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Lazily yields lists of token ids from a streaming HF dataset.\n",
    "    \"\"\"\n",
    "    for ex in hf_dataset:\n",
    "        text = ex.get(\"text\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "        ids = tokenizer.encode(text)\n",
    "        if len(ids) > 1:\n",
    "            yield ids\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def window_stream(token_iter, context_len, device):\n",
    "    \"\"\"\n",
    "    Yields single (x, y) training samples of shape [context_len].\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "\n",
    "    for ids in token_iter:\n",
    "        buffer.extend(ids)\n",
    "\n",
    "        while len(buffer) >= context_len + 1:\n",
    "            start = random.randint(0, len(buffer) - context_len - 1)\n",
    "\n",
    "            x = buffer[start : start + context_len]\n",
    "            y = buffer[start + 1 : start + context_len + 1]\n",
    "\n",
    "            yield (\n",
    "                torch.tensor(x, dtype=torch.long, device=device),\n",
    "                torch.tensor(y, dtype=torch.long, device=device),\n",
    "            )\n",
    "def batch_stream(sample_iter, batch_size):\n",
    "    \"\"\"\n",
    "    Groups single samples into batches.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        xb, yb = zip(*(next(sample_iter) for _ in range(batch_size)))\n",
    "        yield torch.stack(xb), torch.stack(yb)\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load streaming dataset\n",
    "ds = load_dataset(\n",
    "    \"monology/pile-uncopyrighted\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "\n",
    "# build streams\n",
    "tok_iter   = token_stream(ds, tokenizer)\n",
    "sample_iter = window_stream(tok_iter, context_len, device)\n",
    "batch_iter  = batch_stream(sample_iter, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_mytokenyeah\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/345rf4gt56t4r3e3/nnn', endpoint='https://huggingface.co', repo_type='model', repo_id='345rf4gt56t4r3e3/nnn')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, upload_file\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "HF_REPO = \"345rf4gt56t4r3e3/nnn\"\n",
    "CKPT_DIR = \"checkpoints\"\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# create repo if it doesn't exist\n",
    "api.create_repo(\n",
    "    repo_id=HF_REPO,\n",
    "    exist_ok=True,\n",
    "    repo_type=\"model\",\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YxLoyfRq9Lh7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9171/2807709901.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9d140161e646f6a406eebe1511a4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9171/2807709901.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0 | loss 10.9504\n",
      "step    10 | loss 10.9479\n",
      "step    20 | loss 10.9091\n",
      "step    30 | loss 10.8186\n",
      "step    40 | loss 10.7316\n",
      "step    50 | loss 10.5976\n",
      "step    60 | loss 10.4588\n",
      "step    70 | loss 10.2639\n",
      "step    80 | loss 10.0770\n",
      "step    90 | loss 9.8780\n",
      "step   100 | loss 9.7151\n",
      "step   110 | loss 9.5252\n",
      "step   120 | loss 9.3798\n",
      "step   130 | loss 9.1531\n",
      "step   140 | loss 9.0567\n",
      "step   150 | loss 8.9283\n",
      "step   160 | loss 8.7812\n",
      "step   170 | loss 8.7036\n",
      "step   180 | loss 8.7216\n",
      "step   190 | loss 8.4415\n",
      "step   200 | loss 8.3514\n",
      "step   210 | loss 8.4170\n",
      "step   220 | loss 8.3396\n",
      "step   230 | loss 8.2484\n",
      "step   240 | loss 8.1511\n",
      "step   250 | loss 7.9582\n",
      "step   260 | loss 8.0959\n",
      "step   270 | loss 7.9606\n",
      "step   280 | loss 7.8577\n",
      "step   290 | loss 7.7751\n",
      "step   300 | loss 7.6415\n",
      "step   310 | loss 7.5857\n",
      "step   320 | loss 7.5396\n",
      "step   330 | loss 7.5784\n",
      "step   340 | loss 7.3402\n",
      "step   350 | loss 7.3432\n",
      "step   360 | loss 7.2335\n",
      "step   370 | loss 7.3027\n",
      "step   380 | loss 7.0501\n",
      "step   390 | loss 6.8392\n",
      "step   400 | loss 6.8918\n",
      "step   410 | loss 6.5964\n",
      "step   420 | loss 6.5668\n",
      "step   430 | loss 6.5298\n",
      "step   440 | loss 6.3214\n",
      "step   450 | loss 6.0414\n",
      "step   460 | loss 5.9992\n",
      "step   470 | loss 5.7757\n",
      "step   480 | loss 5.9262\n",
      "step   490 | loss 5.6138\n",
      "step   500 | loss 5.4575\n",
      "step   510 | loss 5.2292\n",
      "step   520 | loss 5.2426\n",
      "step   530 | loss 5.0391\n",
      "step   540 | loss 4.6463\n",
      "step   550 | loss 4.6552\n",
      "step   560 | loss 4.4857\n",
      "step   570 | loss 4.7446\n",
      "step   580 | loss 4.2538\n",
      "step   590 | loss 4.6545\n",
      "step   600 | loss 3.8054\n",
      "step   610 | loss 4.0218\n",
      "step   620 | loss 3.9456\n",
      "step   630 | loss 3.7139\n",
      "step   640 | loss 3.6076\n",
      "step   650 | loss 3.8135\n",
      "step   660 | loss 3.4147\n",
      "step   670 | loss 3.3086\n",
      "step   680 | loss 3.6061\n",
      "step   690 | loss 3.4645\n",
      "step   700 | loss 3.7028\n",
      "[saved FP16 weights â†’ weights/model_step_700.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e1734af6ed4eda857525ed5815e7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854d01f7810d40d0b094fae7e69636a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'The read operation timed out' thrown while requesting POST https://huggingface.co/api/models/345rf4gt56t4r3e3/nnn/preupload/main\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaa697059044d6c8fe8ac8130bd761d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd2545cd6404e83acad518518980640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded â†’ hf://345rf4gt56t4r3e3/nnn/model_step_700.pt]\n",
      "step   710 | loss 3.4508\n",
      "step   720 | loss 3.0982\n",
      "step   730 | loss 3.2763\n",
      "step   740 | loss 3.2156\n",
      "step   750 | loss 2.9231\n",
      "step   760 | loss 2.9953\n",
      "step   770 | loss 3.1089\n",
      "step   780 | loss 2.9970\n",
      "step   790 | loss 2.9663\n",
      "step   800 | loss 3.0363\n",
      "step   810 | loss 2.6403\n",
      "step   820 | loss 2.7821\n",
      "step   830 | loss 2.6905\n",
      "step   840 | loss 2.6176\n",
      "step   850 | loss 2.6778\n",
      "step   860 | loss 2.6074\n",
      "step   870 | loss 2.7695\n",
      "step   880 | loss 2.5941\n",
      "step   890 | loss 2.4486\n",
      "step   900 | loss 2.6034\n",
      "step   910 | loss 2.5522\n",
      "step   920 | loss 2.4749\n",
      "step   930 | loss 2.8069\n",
      "step   940 | loss 2.5082\n",
      "step   950 | loss 2.5039\n",
      "step   960 | loss 2.6154\n",
      "step   970 | loss 2.5087\n",
      "step   980 | loss 2.3259\n",
      "step   990 | loss 2.5937\n",
      "step  1000 | loss 2.4408\n",
      "step  1010 | loss 2.3217\n",
      "step  1020 | loss 2.2404\n",
      "step  1030 | loss 2.5770\n",
      "step  1040 | loss 2.4223\n",
      "step  1050 | loss 2.2085\n",
      "step  1060 | loss 2.1079\n",
      "step  1070 | loss 2.2974\n",
      "step  1080 | loss 2.1298\n",
      "step  1090 | loss 2.3676\n",
      "step  1100 | loss 2.2719\n",
      "step  1110 | loss 2.0909\n",
      "step  1120 | loss 2.1623\n",
      "step  1130 | loss 2.2585\n",
      "step  1140 | loss 2.2872\n",
      "step  1150 | loss 1.8876\n",
      "step  1160 | loss 1.9074\n",
      "step  1170 | loss 1.9218\n",
      "step  1180 | loss 2.1878\n",
      "step  1190 | loss 1.8833\n",
      "step  1200 | loss 2.0362\n",
      "step  1210 | loss 2.0271\n",
      "step  1220 | loss 2.0473\n",
      "step  1230 | loss 1.9769\n",
      "step  1240 | loss 1.8482\n",
      "step  1250 | loss 1.8711\n",
      "step  1260 | loss 1.8869\n",
      "step  1270 | loss 1.9126\n",
      "step  1280 | loss 1.7315\n",
      "step  1290 | loss 1.7864\n",
      "step  1300 | loss 1.8429\n",
      "step  1310 | loss 1.9407\n",
      "step  1320 | loss 1.8705\n",
      "step  1330 | loss 1.6261\n",
      "step  1340 | loss 1.6099\n",
      "step  1350 | loss 1.6113\n",
      "step  1360 | loss 1.4867\n",
      "step  1370 | loss 1.5837\n",
      "step  1380 | loss 1.6304\n",
      "step  1390 | loss 1.4487\n",
      "step  1400 | loss 1.6657\n",
      "[saved FP16 weights â†’ weights/model_step_1400.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d33c1370554393b49fd81d726ac57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a527b7985814872b973e86fee7798bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd7f300bc914e309fc54a7df3f198c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9654ca298e4c4898018548a9590bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded â†’ hf://345rf4gt56t4r3e3/nnn/model_step_1400.pt]\n",
      "step  1410 | loss 1.4795\n",
      "step  1420 | loss 1.4394\n",
      "step  1430 | loss 1.7100\n",
      "step  1440 | loss 1.3239\n",
      "step  1450 | loss 1.3800\n",
      "step  1460 | loss 1.4112\n",
      "step  1470 | loss 1.3611\n",
      "step  1480 | loss 1.5482\n",
      "step  1490 | loss 1.3672\n",
      "step  1500 | loss 1.2588\n",
      "step  1510 | loss 1.2188\n",
      "step  1520 | loss 1.4458\n",
      "step  1530 | loss 1.3950\n",
      "step  1540 | loss 1.3066\n",
      "step  1550 | loss 1.1370\n",
      "step  1560 | loss 1.3426\n",
      "step  1570 | loss 1.2565\n",
      "step  1580 | loss 1.1822\n",
      "step  1590 | loss 1.2953\n",
      "step  1600 | loss 1.2644\n",
      "step  1610 | loss 1.4146\n",
      "step  1620 | loss 1.2526\n",
      "step  1630 | loss 1.1391\n",
      "step  1640 | loss 1.1153\n",
      "step  1650 | loss 1.2446\n",
      "step  1660 | loss 0.9831\n",
      "step  1670 | loss 1.1703\n",
      "step  1680 | loss 1.0244\n",
      "step  1690 | loss 1.1867\n",
      "step  1700 | loss 0.9559\n",
      "step  1710 | loss 0.9530\n",
      "step  1720 | loss 0.9545\n",
      "step  1730 | loss 0.9174\n",
      "step  1740 | loss 0.8532\n",
      "step  1750 | loss 0.9469\n",
      "step  1760 | loss 0.9774\n",
      "step  1770 | loss 0.9164\n",
      "step  1780 | loss 0.8931\n",
      "step  1790 | loss 0.8178\n",
      "step  1800 | loss 0.8012\n",
      "step  1810 | loss 0.9408\n",
      "step  1820 | loss 0.7890\n",
      "step  1830 | loss 0.8468\n",
      "step  1840 | loss 0.8651\n",
      "step  1850 | loss 0.7033\n",
      "step  1860 | loss 0.7430\n",
      "step  1870 | loss 0.6841\n",
      "step  1880 | loss 0.6401\n",
      "step  1890 | loss 0.7930\n",
      "step  1900 | loss 0.6776\n",
      "step  1910 | loss 0.6457\n",
      "step  1920 | loss 0.5574\n",
      "step  1930 | loss 0.6994\n",
      "step  1940 | loss 0.6173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b0911e802c4f4fb82124bdf8f9f3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f3183e7c9c4f6b93fe6ab67702fe45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded â†’ hf://345rf4gt56t4r3e3/nnn/model_step_2100.pt]\n",
      "step  2110 | loss 0.4070\n",
      "step  2120 | loss 0.4219\n",
      "step  2130 | loss 0.4588\n",
      "step  2140 | loss 0.3288\n",
      "step  2150 | loss 0.4323\n",
      "step  2160 | loss 0.3760\n",
      "step  2170 | loss 0.3659\n",
      "step  2180 | loss 0.3389\n",
      "step  2190 | loss 0.3540\n",
      "step  2200 | loss 0.2749\n",
      "step  2210 | loss 0.3233\n",
      "step  2220 | loss 0.2835\n",
      "step  2230 | loss 0.3643\n",
      "step  2240 | loss 0.3046\n",
      "step  2250 | loss 0.2563\n",
      "step  2260 | loss 0.2901\n",
      "step  2270 | loss 0.2622\n",
      "step  2280 | loss 0.1930\n",
      "step  2290 | loss 0.2778\n",
      "step  2300 | loss 0.3602\n",
      "step  2310 | loss 0.1758\n",
      "step  2320 | loss 0.2913\n",
      "step  2330 | loss 0.2402\n",
      "step  2340 | loss 0.1749\n",
      "step  2350 | loss 0.2563\n",
      "step  2360 | loss 0.2387\n",
      "step  2370 | loss 0.2438\n",
      "step  2380 | loss 0.2262\n",
      "step  2390 | loss 0.2054\n",
      "step  2400 | loss 0.1955\n",
      "step  2410 | loss 0.2703\n",
      "step  2420 | loss 0.1868\n",
      "step  2430 | loss 0.2155\n",
      "step  2440 | loss 0.1850\n",
      "step  2450 | loss 0.1941\n",
      "step  2460 | loss 0.2285\n",
      "step  2470 | loss 0.1247\n",
      "step  2480 | loss 0.1283\n",
      "step  2490 | loss 0.1605\n",
      "step  2500 | loss 0.1195\n",
      "step  2510 | loss 0.2199\n",
      "step  2520 | loss 0.1655\n",
      "step  2530 | loss 0.1204\n",
      "step  2540 | loss 0.1260\n",
      "step  2550 | loss 0.1987\n",
      "step  2560 | loss 0.1174\n",
      "step  2570 | loss 0.1565\n",
      "step  2580 | loss 0.1342\n",
      "step  2590 | loss 0.1790\n",
      "step  2600 | loss 0.0903\n",
      "step  2610 | loss 0.1250\n",
      "step  2620 | loss 0.1225\n",
      "step  2630 | loss 0.1663\n",
      "step  2640 | loss 0.1595\n",
      "step  2650 | loss 0.0712\n",
      "step  2660 | loss 0.0757\n",
      "step  2670 | loss 0.1026\n",
      "step  2680 | loss 0.1130\n",
      "step  2690 | loss 0.1088\n",
      "step  2700 | loss 0.0797\n",
      "step  2710 | loss 0.0739\n",
      "step  2720 | loss 0.0974\n",
      "step  2730 | loss 0.1119\n",
      "step  2740 | loss 0.0707\n",
      "step  2750 | loss 0.1025\n",
      "step  2760 | loss 0.0910\n",
      "step  2770 | loss 0.0752\n",
      "step  2780 | loss 0.0380\n",
      "step  2790 | loss 0.0540\n",
      "step  2800 | loss 0.0745\n",
      "[saved FP16 weights â†’ weights/model_step_2800.pt]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcbd6b68c9649fbb56217369c34a749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32984e1a4174d6e9910c6ad8090b41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d062022ecf6a4e1198b865ae7a593a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e7a8a3801847568c1c8d59d985025c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[uploaded â†’ hf://345rf4gt56t4r3e3/nnn/model_step_2800.pt]\n",
      "step  2810 | loss 0.0448\n",
      "step  2820 | loss 0.0446\n",
      "step  2830 | loss 0.0760\n",
      "step  2840 | loss 0.0481\n",
      "step  2850 | loss 0.0713\n",
      "step  2860 | loss 0.0621\n",
      "step  2870 | loss 0.0608\n",
      "step  2880 | loss 0.0501\n",
      "step  2890 | loss 0.0514\n",
      "step  2900 | loss 0.0490\n",
      "step  2910 | loss 0.0659\n",
      "step  2920 | loss 0.0693\n",
      "step  2930 | loss 0.0555\n",
      "step  2940 | loss 0.0626\n",
      "step  2950 | loss 0.0675\n",
      "step  2960 | loss 0.0654\n",
      "step  2970 | loss 0.0697\n",
      "step  2980 | loss 0.0748\n",
      "step  2990 | loss 0.0632\n",
      "step  3000 | loss 0.0512\n",
      "step  3010 | loss 0.0416\n",
      "step  3020 | loss 0.0382\n",
      "step  3030 | loss 0.0511\n",
      "step  3040 | loss 0.0438\n",
      "step  3050 | loss 0.0618\n",
      "step  3060 | loss 0.0506\n",
      "step  3070 | loss 0.0395\n",
      "step  3080 | loss 0.0555\n",
      "step  3090 | loss 0.0324\n",
      "step  3100 | loss 0.0258\n",
      "step  3110 | loss 0.0253\n",
      "step  3120 | loss 0.0309\n",
      "step  3130 | loss 0.0278\n",
      "step  3140 | loss 0.0381\n",
      "step  3150 | loss 0.0265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     _, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     38\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "CKPT_DIR   = \"weights\"\n",
    "SAVE_EVERY = 700\n",
    "# ---------------------------\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "max_steps = 9000\n",
    "history = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step in tqdm(range(max_steps)):\n",
    "    # ---- LR schedule ----\n",
    "    lr = get_lr(step, max_steps)\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = lr\n",
    "\n",
    "    # ---- get batch ----\n",
    "    xb, yb = next(batch_iter)\n",
    "    xb = xb.to(device, non_blocking=True)\n",
    "    yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "    # ---- forward + backward (AMP) ----\n",
    "    with torch.cuda.amp.autocast():\n",
    "        _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    # ---- logging ----\n",
    "    loss_val = loss.item()\n",
    "    history.append(loss_val)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step:5d} | loss {loss_val:.4f}\")\n",
    "\n",
    "    # ---- SAVE *WEIGHTS ONLY* (SAFE) ----\n",
    "    if step > 0 and step % SAVE_EVERY == 0:\n",
    "        fname = f\"model_step_{step}.pt\"\n",
    "        fpath = os.path.join(CKPT_DIR, fname)\n",
    "\n",
    "        # ðŸ”’ SAFE FP16 EXPORT (does NOT touch training model)\n",
    "        state_fp16 = {\n",
    "            k: v.detach().half().cpu()\n",
    "            for k, v in model.state_dict().items()\n",
    "        }\n",
    "\n",
    "        torch.save(state_fp16, fpath)\n",
    "        print(f\"[saved FP16 weights â†’ {fpath}]\")\n",
    "\n",
    "        # upload versioned snapshot\n",
    "        upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=fname,\n",
    "            repo_id=HF_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"weights @ step {step}\",\n",
    "        )\n",
    "\n",
    "        # update rolling pointer\n",
    "        upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=\"latest.pt\",\n",
    "            repo_id=HF_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"update latest @ step {step}\",\n",
    "        )\n",
    "\n",
    "        print(f\"[uploaded â†’ hf://{HF_REPO}/{fname}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8GG-GT7_Pwz"
   },
   "outputs": [],
   "source": [
    "print(xb[0][:10])\n",
    "print(yb[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "h2mciRDW_711"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def complete(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # encode prompt\n",
    "    idx = torch.tensor(\n",
    "        [tokenizer.encode(prompt)],\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop context if needed\n",
    "        idx_cond = idx[:, -context_len :]\n",
    "\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # decode only the completion\n",
    "    completion = tokenizer.decode(idx[0].tolist())\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Fnuym0M-dgMX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is a color denotes the longest time. It was great seeing other people working â€“ I had a few tabs opened on my second monitor all the time. Itâ€™s actually a bit sad, because if I could, I could have spent the whole weekend just watching other people working! But I had to do my\n"
     ]
    }
   ],
   "source": [
    "from tiktoken import get_encoding\n",
    "\n",
    "\n",
    "text = complete(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=\"Blue is a color\",\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlhiyAIreTvx"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh96fd9xec7v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
