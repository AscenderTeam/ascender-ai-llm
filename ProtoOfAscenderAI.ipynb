{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLAWn2V4ZpIn"
   },
   "source": [
    "# 500M transformer based on pile uncopyrighted by stream training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo5lerbom6dj",
    "outputId": "6f5855ee-fe33-465f-beb2-41184cd9bd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
      "Requirement already satisfied: fsspec[compression] in /usr/local/lib/python3.11/dist-packages (2024.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "\u001b[33mWARNING: fsspec 2024.2.0 does not provide the extra 'compression'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken tqdm datasets tiktoken zstandard \"fsspec[compression]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvKFeUoRngtR"
   },
   "source": [
    "# RESTART THE KERNEL IF RUNNING ON RUNPOD AFTER THIS PIP INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9enT73mnYoiM"
   },
   "outputs": [],
   "source": [
    "# -------- MODEL PARAMS --------\n",
    "n_layers    = 64\n",
    "n_embd      = 2048\n",
    "n_heads     = 16\n",
    "context_len = 1024\n",
    "batch_size  = 8    # with grad accumulation\n",
    "dropout     = 0\n",
    "lr          = 3e-6\n",
    "from tiktoken import get_encoding\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "vocab_size  = tokenizer.n_vocab\n",
    "\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxN4QEpOCCHN"
   },
   "source": [
    "# *cool lr sinusoid trick*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AQ3o30qoZIW6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr(step, max_steps, base_lr=lr, warmup_steps=2000):\n",
    "    \"\"\"\n",
    "    Warmup + cosine decay learning rate schedule.\n",
    "\n",
    "    Args:\n",
    "        step (int): current training step\n",
    "        max_steps (int): total training steps\n",
    "        base_lr (float): peak learning rate\n",
    "        warmup_steps (int): number of warmup steps\n",
    "\n",
    "    Returns:\n",
    "        float: learning rate for this step\n",
    "    \"\"\"\n",
    "    if step < warmup_steps:\n",
    "        return base_lr * step / warmup_steps\n",
    "\n",
    "    progress = (step - warmup_steps) / max(1, max_steps - warmup_steps)\n",
    "    return base_lr * 0.5 * (1.0 + math.cos(math.pi * progress))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YNClLsseWNq-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "478tzhmPyb8g"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token(x)                    # (B, T, C)\n",
    "        pos = self.pos(torch.arange(T))        # (T, C)\n",
    "        return tok + pos\n",
    "emb = TokenEmbedding()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YL_tG3Xy1Bk"
   },
   "source": [
    "# Above is the definition of our embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ugc-43hy6Ak"
   },
   "source": [
    "# Now define attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tW4hfpmPyh-0"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)                       # (B, T, 3C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # this is the critical line\n",
    "        y = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            is_causal=True,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "        )\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO3ojwF2yvFb"
   },
   "source": [
    "# Constants again and positional embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-TUsNPmp2iWA"
   },
   "outputs": [],
   "source": [
    "class TokenPosEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token(x)                                # (B,T,C)\n",
    "        pos = self.pos(torch.arange(T, device=x.device))  # (T,C)\n",
    "        return tok + pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Jj6mrYi7M9Z"
   },
   "source": [
    "# After attention we have basic FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AIBpH7V57QfZ"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rdJuWGal6g6Z"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n",
    "        self.ff   = FeedForward()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "O59GSjd07CcU"
   },
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # token + positional embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb   = nn.Embedding(context_len, n_embd)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block() for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # final normalization + LM head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        # embeddings\n",
    "        tok = self.token_emb(x)                              # (B,T,C)\n",
    "        pos = self.pos_emb(torch.arange(T, device=x.device))# (T,C)\n",
    "        x = tok + pos                                        # (B,T,C)\n",
    "\n",
    "        # APPLY ALL BLOCKS HERE\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # final projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                                # (B,T,vocab)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits = logits.view(B*T, vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w9471Vfr8CrW"
   },
   "outputs": [],
   "source": [
    "model = TransformerLM().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oWWhQhrG8pj8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3430.409297 M params\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M params\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xakqTi7NCJsq"
   },
   "source": [
    "# optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "36Uwu8ox8yXJ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2LPa6zEb88eE"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        losses = []\n",
    "        for _ in range(20):\n",
    "            xb, yb = get_batch(split)\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train() # go back because we will be in a train loop\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zPfmgofSbChr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62b8a79516146babff329f6087bd885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3977226db3834d4bba8d8472bee7d9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def token_stream(hf_dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Lazily yields lists of token ids from a streaming HF dataset.\n",
    "    \"\"\"\n",
    "    for ex in hf_dataset:\n",
    "        text = ex.get(\"text\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "        ids = tokenizer.encode(text)\n",
    "        if len(ids) > 1:\n",
    "            yield ids\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def window_stream(token_iter, context_len, device):\n",
    "    \"\"\"\n",
    "    Yields single (x, y) training samples of shape [context_len].\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "\n",
    "    for ids in token_iter:\n",
    "        buffer.extend(ids)\n",
    "\n",
    "        while len(buffer) >= context_len + 1:\n",
    "            start = random.randint(0, len(buffer) - context_len - 1)\n",
    "\n",
    "            x = buffer[start : start + context_len]\n",
    "            y = buffer[start + 1 : start + context_len + 1]\n",
    "\n",
    "            yield (\n",
    "                torch.tensor(x, dtype=torch.long, device=device),\n",
    "                torch.tensor(y, dtype=torch.long, device=device),\n",
    "            )\n",
    "def batch_stream(sample_iter, batch_size):\n",
    "    \"\"\"\n",
    "    Groups single samples into batches.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        xb, yb = zip(*(next(sample_iter) for _ in range(batch_size)))\n",
    "        yield torch.stack(xb), torch.stack(yb)\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load streaming dataset\n",
    "ds = load_dataset(\n",
    "    \"monology/pile-uncopyrighted\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = get_encoding(\"gpt2\")\n",
    "\n",
    "# build streams\n",
    "tok_iter   = token_stream(ds, tokenizer)\n",
    "sample_iter = window_stream(tok_iter, context_len, device)\n",
    "batch_iter  = batch_stream(sample_iter, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_yeahatokenhere\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/345rf4gt56t4r3e3/nnn', endpoint='https://huggingface.co', repo_type='model', repo_id='345rf4gt56t4r3e3/nnn')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, upload_file\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "HF_REPO = \"345rf4gt56t4r3e3/nnn\"\n",
    "CKPT_DIR = \"checkpoints\"\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# create repo if it doesn't exist\n",
    "api.create_repo(\n",
    "    repo_id=HF_REPO,\n",
    "    exist_ok=True,\n",
    "    repo_type=\"model\",\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxLoyfRq9Lh7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2687/3163600894.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfee79978afe4c4bacf1989d3916320f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2687/3163600894.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0 | loss 11.0095\n",
      "step    10 | loss 10.9892\n",
      "step    20 | loss 10.9466\n",
      "step    30 | loss 10.8715\n",
      "step    40 | loss 10.7718\n",
      "step    50 | loss 10.6325\n",
      "step    60 | loss 10.4818\n",
      "step    70 | loss 10.2661\n",
      "step    80 | loss 10.1145\n",
      "step    90 | loss 9.8985\n",
      "step   100 | loss 9.6991\n",
      "step   110 | loss 9.5507\n",
      "step   120 | loss 9.3283\n",
      "step   130 | loss 9.1491\n",
      "step   140 | loss 9.0098\n",
      "step   150 | loss 8.9417\n",
      "step   160 | loss 8.7541\n",
      "step   170 | loss 8.6928\n",
      "step   180 | loss 8.5124\n",
      "step   190 | loss 8.5088\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "CKPT_DIR   = \"weights\"\n",
    "SAVE_EVERY = 300\n",
    "# ---------------------------\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "max_steps = 9000\n",
    "history = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step in tqdm(range(max_steps)):\n",
    "    # ---- LR schedule ----\n",
    "    lr = get_lr(step, max_steps)\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = lr\n",
    "\n",
    "    # ---- get batch ----\n",
    "    xb, yb = next(batch_iter)\n",
    "    xb = xb.to(device, non_blocking=True)\n",
    "    yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "    # ---- forward + backward (AMP) ----\n",
    "    with torch.cuda.amp.autocast():\n",
    "        _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    # ---- logging ----\n",
    "    loss_val = loss.item()\n",
    "    history.append(loss_val)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step:5d} | loss {loss_val:.4f}\")\n",
    "\n",
    "    # ---- SAVE *WEIGHTS ONLY* ----\n",
    "    if step > 0 and step % SAVE_EVERY == 0:\n",
    "        fname = f\"model_step_{step}.pt\"\n",
    "        fpath = os.path.join(CKPT_DIR, fname)\n",
    "\n",
    "        # ensure inference-friendly weights\n",
    "        model_to_save = model.half().cpu()\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), fpath)\n",
    "        print(f\"[saved weights → {fpath}]\")\n",
    "\n",
    "        # restore training state\n",
    "        model.to(device).train()\n",
    "\n",
    "        # upload versioned snapshot\n",
    "        upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=fname,\n",
    "            repo_id=HF_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"weights @ step {step}\",\n",
    "        )\n",
    "\n",
    "        # update rolling pointer\n",
    "        upload_file(\n",
    "            path_or_fileobj=fpath,\n",
    "            path_in_repo=\"latest.pt\",\n",
    "            repo_id=HF_REPO,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"update latest @ step {step}\",\n",
    "        )\n",
    "\n",
    "        print(f\"[uploaded → hf://{HF_REPO}/{fname}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8GG-GT7_Pwz"
   },
   "outputs": [],
   "source": [
    "print(xb[0][:10])\n",
    "print(yb[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2mciRDW_711"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def complete(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = 50,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # encode prompt\n",
    "    idx = torch.tensor(\n",
    "        [tokenizer.encode(prompt)],\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop context if needed\n",
    "        idx_cond = idx[:, -context_len :]\n",
    "\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # decode only the completion\n",
    "    completion = tokenizer.decode(idx[0].tolist())\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fnuym0M-dgMX"
   },
   "outputs": [],
   "source": [
    "from tiktoken import get_encoding\n",
    "\n",
    "\n",
    "text = complete(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=\"Blue is a\",\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.02,\n",
    "    top_k=40,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlhiyAIreTvx"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh96fd9xec7v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
