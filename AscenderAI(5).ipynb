{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3.4B transformer based on pile uncopyrighted and 40 gig of data"
      ],
      "metadata": {
        "id": "HLAWn2V4ZpIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tiktoken tqdm datasets tiktoken zstandard \"fsspec[compression]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo5lerbom6dj",
        "outputId": "6f5855ee-fe33-465f-beb2-41184cd9bd01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "Requirement already satisfied: fsspec[compression] in /usr/local/lib/python3.12/dist-packages (2025.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "\u001b[33mWARNING: fsspec 2025.3.0 does not provide the extra 'compression'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESTART THE KERNEL IF RUNNING ON RUNPOD AFTER THIS PIP INSTALL"
      ],
      "metadata": {
        "id": "XvKFeUoRngtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- MODEL PARAMS --------\n",
        "n_layers    = 64\n",
        "n_embd      = 2048\n",
        "n_heads     = 16\n",
        "context_len = 1024\n",
        "batch_size  = 8    # with grad accumulation\n",
        "dropout     = 0\n",
        "lr          = 3e-6\n",
        "from tiktoken import get_encoding\n",
        "tokenizer = get_encoding(\"gpt2\")\n",
        "vocab_size  = tokenizer.n_vocab\n",
        "\n",
        "# ------------------------------"
      ],
      "metadata": {
        "id": "9enT73mnYoiM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *cool lr sinusoid trick*"
      ],
      "metadata": {
        "id": "SxN4QEpOCCHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_lr(step, max_steps, base_lr=lr, warmup_steps=2000):\n",
        "    \"\"\"\n",
        "    Warmup + cosine decay learning rate schedule.\n",
        "\n",
        "    Args:\n",
        "        step (int): current training step\n",
        "        max_steps (int): total training steps\n",
        "        base_lr (float): peak learning rate\n",
        "        warmup_steps (int): number of warmup steps\n",
        "\n",
        "    Returns:\n",
        "        float: learning rate for this step\n",
        "    \"\"\"\n",
        "    if step < warmup_steps:\n",
        "        return base_lr * step / warmup_steps\n",
        "\n",
        "    progress = (step - warmup_steps) / max(1, max_steps - warmup_steps)\n",
        "    return base_lr * 0.5 * (1.0 + math.cos(math.pi * progress))\n"
      ],
      "metadata": {
        "id": "AQ3o30qoZIW6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "YNClLsseWNq-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos   = nn.Embedding(context_len, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        tok = self.token(x)                    # (B, T, C)\n",
        "        pos = self.pos(torch.arange(T))        # (T, C)\n",
        "        return tok + pos\n",
        "emb = TokenEmbedding()\n",
        "\n"
      ],
      "metadata": {
        "id": "478tzhmPyb8g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Above is the definition of our embeddings"
      ],
      "metadata": {
        "id": "5YL_tG3Xy1Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now define attention"
      ],
      "metadata": {
        "id": "5Ugc-43hy6Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_heads == 0\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = n_embd // n_heads\n",
        "\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
        "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)                       # (B, T, 3C)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # this is the critical line\n",
        "        y = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            is_causal=True,\n",
        "            dropout_p=self.dropout if self.training else 0.0,\n",
        "        )\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.proj(y)\n"
      ],
      "metadata": {
        "id": "tW4hfpmPyh-0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants again and positional embeddings\n"
      ],
      "metadata": {
        "id": "gO3ojwF2yvFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenPosEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos   = nn.Embedding(context_len, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        tok = self.token(x)                                # (B,T,C)\n",
        "        pos = self.pos(torch.arange(T, device=x.device))  # (T,C)\n",
        "        return tok + pos\n"
      ],
      "metadata": {
        "id": "-TUsNPmp2iWA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After attention we have basic FFN"
      ],
      "metadata": {
        "id": "2Jj6mrYi7M9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "AIBpH7V57QfZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        self.attn = CausalSelfAttention(n_embd, n_heads, dropout)\n",
        "        self.ff   = FeedForward()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rdJuWGal6g6Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # token + positional embeddings\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb   = nn.Embedding(context_len, n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block() for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        # final normalization + LM head\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        B, T = x.shape\n",
        "\n",
        "        # embeddings\n",
        "        tok = self.token_emb(x)                              # (B,T,C)\n",
        "        pos = self.pos_emb(torch.arange(T, device=x.device))# (T,C)\n",
        "        x = tok + pos                                        # (B,T,C)\n",
        "\n",
        "        # APPLY ALL BLOCKS HERE\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # final projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)                                # (B,T,vocab)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits = logits.view(B*T, vocab_size)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n"
      ],
      "metadata": {
        "id": "O59GSjd07CcU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerLM().to(device)\n"
      ],
      "metadata": {
        "id": "w9471Vfr8CrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M params\")\n"
      ],
      "metadata": {
        "id": "oWWhQhrG8pj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sk4ZHJ2fCFIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# optimus"
      ],
      "metadata": {
        "id": "xakqTi7NCJsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=lr\n",
        ")\n"
      ],
      "metadata": {
        "id": "36Uwu8ox8yXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in [\"train\", \"test\"]:\n",
        "        losses = []\n",
        "        for _ in range(20):\n",
        "            xb, yb = get_batch(split)\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "        out[split] = sum(losses) / len(losses)\n",
        "    model.train() # go back because we will be in a train loop\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "2LPa6zEb88eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_stream(hf_dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Lazily yields lists of token ids from a streaming HF dataset.\n",
        "    \"\"\"\n",
        "    for ex in hf_dataset:\n",
        "        text = ex.get(\"text\", \"\")\n",
        "        if not text:\n",
        "            continue\n",
        "        ids = tokenizer.encode(text)\n",
        "        if len(ids) > 1:\n",
        "            yield ids\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def window_stream(token_iter, context_len, device):\n",
        "    \"\"\"\n",
        "    Yields single (x, y) training samples of shape [context_len].\n",
        "    \"\"\"\n",
        "    buffer = []\n",
        "\n",
        "    for ids in token_iter:\n",
        "        buffer.extend(ids)\n",
        "\n",
        "        while len(buffer) >= context_len + 1:\n",
        "            start = random.randint(0, len(buffer) - context_len - 1)\n",
        "\n",
        "            x = buffer[start : start + context_len]\n",
        "            y = buffer[start + 1 : start + context_len + 1]\n",
        "\n",
        "            yield (\n",
        "                torch.tensor(x, dtype=torch.long, device=device),\n",
        "                torch.tensor(y, dtype=torch.long, device=device),\n",
        "            )\n",
        "def batch_stream(sample_iter, batch_size):\n",
        "    \"\"\"\n",
        "    Groups single samples into batches.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        xb, yb = zip(*(next(sample_iter) for _ in range(batch_size)))\n",
        "        yield torch.stack(xb), torch.stack(yb)\n",
        "from datasets import load_dataset\n",
        "\n",
        "# load streaming dataset\n",
        "ds = load_dataset(\n",
        "    \"monology/pile-uncopyrighted\",\n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        ")\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = get_encoding(\"gpt2\")\n",
        "\n",
        "# build streams\n",
        "tok_iter   = token_stream(ds, tokenizer)\n",
        "sample_iter = window_stream(tok_iter, context_len, device)\n",
        "batch_iter  = batch_stream(sample_iter, batch_size)\n"
      ],
      "metadata": {
        "id": "zPfmgofSbChr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---- checkpoint setup ----\n",
        "CKPT_DIR = \"checkpoints\"\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "max_steps = 9000\n",
        "history = []\n",
        "\n",
        "model.train()\n",
        "\n",
        "for step in tqdm(range(max_steps)):\n",
        "    # ---- LR schedule ----\n",
        "    lr = get_lr(step, max_steps)\n",
        "    for g in optimizer.param_groups:\n",
        "        g[\"lr\"] = lr\n",
        "\n",
        "    # ---- get batch (CPU tensors) ----\n",
        "    xb, yb = next(batch_iter)\n",
        "\n",
        "    # move to GPU ONCE per batch\n",
        "    xb = xb.to(device, non_blocking=True)\n",
        "    yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "    # ---- forward + backward (AMP) ----\n",
        "    with torch.cuda.amp.autocast():\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # ---- logging ----\n",
        "    loss_val = loss.item()\n",
        "    history.append(loss_val)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        print(f\"step {step:5d} | loss {loss_val:.4f}\")\n",
        "\n",
        "    # ---- checkpoint every 300 steps ----\n",
        "    if step > 0 and step % 300 == 0:\n",
        "        ckpt_path = os.path.join(CKPT_DIR, f\"ckpt_step_{step}.pt\")\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"scaler\": scaler.state_dict(),\n",
        "                \"step\": step,\n",
        "            },\n",
        "            ckpt_path,\n",
        "        )\n",
        "        print(f\"[checkpoint saved â†’ {ckpt_path}]\")\n"
      ],
      "metadata": {
        "id": "YxLoyfRq9Lh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb[0][:10])\n",
        "print(yb[0][:10])\n"
      ],
      "metadata": {
        "id": "R8GG-GT7_Pwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def complete(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 50,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int | None = 50,\n",
        "    device: str = \"cuda\",\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    # encode prompt\n",
        "    idx = torch.tensor(\n",
        "        [tokenizer.encode(prompt)],\n",
        "        dtype=torch.long,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop context if needed\n",
        "        idx_cond = idx[:, -context_len :]\n",
        "\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "    # decode only the completion\n",
        "    completion = tokenizer.decode(idx[0].tolist())\n",
        "    return completion\n"
      ],
      "metadata": {
        "id": "h2mciRDW_711"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tiktoken import get_encoding\n",
        "\n",
        "\n",
        "text = complete(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt=\"Hi,\",\n",
        "    max_new_tokens=60,\n",
        "    temperature=0.2,\n",
        "    top_k=40,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "Fnuym0M-dgMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "AlhiyAIreTvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qh96fd9xec7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
